{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stacking",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tetrar124/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/stacking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkYUIzoMjcqW",
        "colab_type": "code",
        "outputId": "892d42eb-22e1-4037-9d43-948e0ee00625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "!pip install fastFM\n",
        "!pip install -U mlxtend\n",
        "!pip install -U pandas\n",
        "!pip install rgf_python\n",
        "#!pip install -U tensorflow-gpu\n",
        "!pip install rgf-python\n",
        "!pip install bayesian-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastFM in /usr/local/lib/python3.6/dist-packages (0.2.11)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fastFM) (0.29.13)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastFM) (0.21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastFM) (1.16.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastFM) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fastFM) (0.13.2)\n",
            "Requirement already up-to-date: mlxtend in /usr/local/lib/python3.6/dist-packages (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.24.2 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.25.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.21.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.13.2 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.13.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.2 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from mlxtend) (41.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->mlxtend) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->mlxtend) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.4.2)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->mlxtend) (1.12.0)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (0.25.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: rgf_python in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from rgf_python) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->rgf_python) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->rgf_python) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->rgf_python) (1.3.1)\n",
            "Requirement already satisfied: rgf-python in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from rgf-python) (0.21.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->rgf-python) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->rgf-python) (1.16.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->rgf-python) (0.13.2)\n",
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.3.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.16.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.13.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBy40_OMlNFA",
        "colab_type": "code",
        "outputId": "a2aaaea9-fe05-4ae8-cc64-f04d14db83af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/colab/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq_yKxUzlg7B",
        "colab_type": "code",
        "outputId": "2c2e1696-c9e6-43ef-d9bc-429bd5512500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "#from fastFM import sgd,als\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.model_selection import GridSearchCV,  cross_validate, StratifiedKFold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from mlxtend.regressor import StackingRegressor\n",
        "from mlxtend.feature_selection import ColumnSelector\n",
        "#from rgf.sklearn import RGFRegressor\n",
        "import xgboost\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from PIL import Image\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras.preprocessing.image import array_to_img\n",
        "from tensorflow.python.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.python.keras.preprocessing.image import load_img\n",
        "from tensorflow.python.keras.layers import Conv1D, MaxPooling1D,BatchNormalization,concatenate,add\n",
        "from tensorflow.python.keras.layers import Activation, Dropout, Flatten, Input, Dense, LSTM,CuDNNLSTM,LSTM,Concatenate,Add,Embedding\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping\n",
        "from tensorflow.python.keras import applications\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin,RegressorMixin\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import scipy as sp\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.decomposition import PCA\n",
        "os.chdir(r'/content/drive/My Drive/Data/Meram Chronic Data')\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#from bayes_opt import BayesianOptimization"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAwtB5XPmTdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ejectCAS = ['10124-36-4', '108-88-3', '111991-09-4', '116-29-0', '120-12-7', '126833-17-8', '13171-21-6',\n",
        "                    '1333-82-0', '137-30-4', '148-79-8', '1582-09-8', '1610-18-0', '2058-46-0', '2104-64-5',\n",
        "                    '21725-46-2',\n",
        "                    '2303-17-5', '25311-71-1', '25812-30-0', '298-00-0', '298-04-4', '314-40-9', '330-54-1',\n",
        "                    '4170-30-3',\n",
        "                    '4717-38-8', '50-00-0', '52645-53-1', '55406-53-6', '56-35-9', '56-38-2', '60207-90-1', '6051-87-2',\n",
        "                    '62-53-3', '6317-18-6', '69-72-7', '7440-02-0', '7447-40-7', '7722-84-1', '7733-02-0', '7758-94-3',\n",
        "                    '80844-07-1', '82657-04-3', '84852-15-3', '86-73-7', '9016-45-9', '99-35-4']\n",
        "\n",
        "df =pd.read_csv('fishMorganMACCS.csv')\n",
        "#df2=pd.read_csv('chronicMACCSKeys_tanimoto.csv')\n",
        "#df2 = df2.drop(ejectCAS,axis=1).set_index('CAS').dropna(how='all', axis=1)\n",
        "baseDf = df\n",
        "extractDf =  df['CAS'].isin(ejectCAS)\n",
        "df = df[~df['CAS'].isin(ejectCAS)]\n",
        "#df = df.set_index('CAS')\n",
        "#df = pd.concat([df,df2],axis=1, join_axes=[df.index]).reset_index()\n",
        "y = df['logTox']\n",
        "#dropList = ['CAS','toxValue','logTox','HDonor', 'HAcceptors', 'AromaticHeterocycles', 'AromaticCarbocycles', 'FractionCSP3']\n",
        "dropList = ['CAS','toxValue','logTox']\n",
        "X = df.drop(columns=dropList)\n",
        "#Normalize\n",
        "def normalize(X):\n",
        "    changeList = []\n",
        "    for i,name in enumerate(X.columns):\n",
        "        if i <679:\n",
        "            changeList.append((0,1))\n",
        "        elif i > 692:\n",
        "            changeList.append((0,1))\n",
        "        else:\n",
        "            #try:\n",
        "            #name = float(name)\n",
        "            #except:\n",
        "            std =X[name].std()\n",
        "            mean = X[name].mean()\n",
        "            if std==0:\n",
        "              X[name] = X[name]\n",
        "            else:\n",
        "              X[name] = X[name].apply(lambda x: ((x - mean) * 1 / std + 0))\n",
        "            changeList.append((mean, std))\n",
        "    return X, changeList\n",
        "X2,_ = normalize(X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RN5BGaXwSy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def dnnCalcACC(testmodel,X=X,name=None):\n",
        "        def calcRMSE(real, pred):\n",
        "            RMSE = (np.sum((pred - real.tolist()) ** 2) / len(pred)) ** (1 / 2)\n",
        "            return RMSE\n",
        "        def calcCorr(real, pred):\n",
        "            corr = np.corrcoef(real, pred.flatten())[0, 1]\n",
        "            return corr\n",
        "        from sklearn.metrics import make_scorer\n",
        "        myScoreFunc = {'RMSE': make_scorer(calcRMSE),\n",
        "                       'Correlation coefficient': make_scorer(calcCorr)}\n",
        "        cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "        Scores = cross_validate(testmodel, X, y, cv=cv, scoring=myScoreFunc,return_train_score=True)\n",
        "        RMSETmp = Scores['test_RMSE'].mean()\n",
        "        CORRTmP = Scores['test_Correlation coefficient'].mean()\n",
        "        trainRMSETmp = Scores['train_RMSE'].mean()\n",
        "        trainCORRTmP = Scores['train_Correlation coefficient'].mean()\n",
        "        print(name,'test', RMSETmp, CORRTmP)\n",
        "        print(name,'train',trainRMSETmp, trainCORRTmP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESdy3BYhBpMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def LSTMmodel(X2=X2):\n",
        "    inputs = Input((X2.shape[1],1))\n",
        "    x = CuDNNLSTM(32,return_sequences=True)(inputs)\n",
        "    #x = CuDNNLSTM(64,return_sequences=True)(x)\n",
        "    x = Flatten()(x)\n",
        "    x  = Dense(1024, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x  = Dense(1024, activation=\"relu\")(x)\n",
        "    predictions = Dense(1)(x)\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "    model.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJSKn4l5EkkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Dense_model(data= X2):\n",
        "  #data = np.expand_dims(data, axis=0)\n",
        "  #data = np.reshape(X_train.values, (-1, 13, 1))\n",
        "  inputs = Input(data.shape[1])\n",
        "  x = Dense(2048, activation=\"relu\")(inputs)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(2048, activation=\"relu\")(x)\n",
        "  predictions = Dense(1)(x)\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "  model.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
        "  #model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u5xSQkHj4B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1d_model(data= X2):\n",
        "  inputs = Input(((data.shape[1],1)))\n",
        "#   print(inputs)\n",
        "  x = Conv1D(8,16,padding=\"same\", activation=\"relu\")(inputs)\n",
        "  x = Conv1D(16,32,padding=\"same\", activation=\"relu\")(x)\n",
        "  x = Conv1D(32,64,padding=\"same\", activation=\"relu\")(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1024, activation=\"relu\")(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(1024, activation=\"relu\")(x)\n",
        "  predictions = Dense(1)(x)\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "  model.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
        "  #model.summary()\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf8CE4beeeTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "outputId": "022ce1d4-f607-484c-cd93-e824fea7b4da"
      },
      "source": [
        "def Conv1d_Dense_model(data= X2):\n",
        "  inputs = Input(((data.shape[1],1)))\n",
        "  x = Conv1D(8,16,padding=\"same\", activation=\"relu\")(inputs)\n",
        "  x = Conv1D(16,32,padding=\"same\", activation=\"relu\")(x)\n",
        "  x = Conv1D(32,64,padding=\"same\", activation=\"relu\")(x)\n",
        "  x1 = Flatten()(x)\n",
        "  x2 = Input(data.shape[1])\n",
        "\n",
        "  x = concatenate([x1,x2],axis=-1) \n",
        "  x = Dense(1024, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(1024, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "  predictions = Dense(1)(x)\n",
        "  \n",
        "  model = Model(inputs=[inputs,x2], outputs=predictions)\n",
        "  model.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
        "  model.summary()\n",
        "  \n",
        "  return model\n",
        "Conv1d_Dense_model()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0822 06:31:40.108533 139659445778304 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 5610, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 5610, 8)      136         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 5610, 16)     4112        conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 5610, 32)     32800       conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 179520)       0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 5610)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 185130)       0           flatten[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1024)         189574144   concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1024)         0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         1049600     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            1025        dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 190,661,817\n",
            "Trainable params: 190,661,817\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.training.Model at 0x7f049baad240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ffvYXigk6p",
        "colab_type": "code",
        "outputId": "6459dca5-5984-425d-f813-1a052a591397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "        cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "        X2 = np.array(X)\n",
        "        y2 = np.array(y)\n",
        "        def calcRMSE(real, pred):\n",
        "            RMSE = (np.sum((pred - real.tolist()) ** 2) / len(pred)) ** (1 / 2)\n",
        "            return RMSE\n",
        "        def calcCorr(real, pred):\n",
        "            corr = np.corrcoef(real, pred.flatten())[0, 1]\n",
        "            return corr\n",
        "        print(X2.shape)\n",
        "        es = EarlyStopping(monitor='val_loss', patience=50, verbose=1, mode='auto')\n",
        "        folds = list(cv.split(X2))\n",
        "        resultscores = []\n",
        "        RMSEresult = []\n",
        "        CorrResult = []\n",
        "        RMSEtest = []\n",
        "        Corrtest = []\n",
        "#         model = LSTMmodel()\n",
        "#         model = Conv1d_model()\n",
        "        model = Conv1d_Dense_model()\n",
        "\n",
        "#         model = Dense_model()\n",
        "         \n",
        "        init_weights = model.get_weights()\n",
        "\n",
        "        for i,(train, test) in enumerate(cv.split(X2)):\n",
        "          print('fold:',i)\n",
        "          ## Need add DIM\n",
        "          model.fit([np.expand_dims(X2[train], axis=2),X2[train]],y2[train],batch_size=256, epochs=100,validation_data=([np.expand_dims(X2[test],axis=2),X2[test]], y2[test]),callbacks=[es])\n",
        "          predict = model.predict([np.expand_dims(X2[test],axis=2),X2[test]])\n",
        "          predictTrain = model.predict([np.expand_dims(X2[train],axis=2),X2[train]])\n",
        "\n",
        "          #model.fit(np.expand_dims(X2[train], axis=2),y2[train],batch_size=256, epochs=100,validation_data=(np.expand_dims(X2[test],axis=2), y2[test]),callbacks=[es])\n",
        "          #predict = model.predict(np.expand_dims(X2[test],axis=2))\n",
        "          #score = model.evaluate([np.expand_dims(X2[test],axis=2), y2[test])\n",
        "\n",
        "          \n",
        "          ## Dense\n",
        "#           model.fit(X2[train],y2[train],batch_size=256, epochs=100,validation_data=(X2[test], y2[test]),callbacks=[es])\n",
        "#           predict = model.predict(X2[test])\n",
        "\n",
        "          predict = predict.flatten()\n",
        "          predictTrain = predictTrain.flatten()\n",
        "\n",
        "          rmse = calcRMSE(y2[test],predict)\n",
        "          corr = calcCorr(y2[test],predict)\n",
        "          rmseTrain = calcRMSE(y2[train],predictTrain)\n",
        "          corrTrain = calcCorr(y2[train],predictTrain)\n",
        "          RMSEresult.append(rmse)\n",
        "          CorrResult.append(corr)\n",
        "          RMSEtest.append(rmseTrain)\n",
        "          Corrtest.append(corrTrain)\n",
        "          print('Test Corr:',corr)\n",
        "          print('Test RMSE:',rmse)\n",
        "          print('Train Corr:',corrTrain)\n",
        "          print('Train RMSE:',rmseTrain)\n",
        "          #           score = model.evaluate(X2[test], y2[test])\n",
        "          #print(\"%s: %.2f%%\" % (model.metrics_names[0], score))\n",
        "          #resultscores.append(score)\n",
        "          model.set_weights(init_weights)\n",
        "          \n",
        "          \n",
        "        print('RMSE result:',sum(RMSEresult)/len(RMSEresult))\n",
        "        print('Corr result:',sum(CorrResult)/len(CorrResult))\n",
        "        print('RMSE train:',sum(RMSEtest)/len(RMSEtest))\n",
        "        print('Corr train:',sum(Corrtest)/len(Corrtest))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1254, 5610)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 5610, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 5610, 8)      136         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 5610, 16)     4112        conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 5610, 32)     32800       conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 179520)       0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 5610)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 185130)       0           flatten_1[0][0]                  \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1024)         189574144   concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1024)         1049600     dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            1025        dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 190,661,817\n",
            "Trainable params: 190,661,817\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "fold: 0\n",
            "Train on 1128 samples, validate on 126 samples\n",
            "Epoch 1/100\n",
            "1128/1128 [==============================] - 16s 14ms/sample - loss: 14.9264 - val_loss: 5.4069\n",
            "Epoch 2/100\n",
            "1128/1128 [==============================] - 1s 942us/sample - loss: 4.9596 - val_loss: 5.2327\n",
            "Epoch 3/100\n",
            "1128/1128 [==============================] - 1s 946us/sample - loss: 4.5319 - val_loss: 4.8462\n",
            "Epoch 4/100\n",
            "1128/1128 [==============================] - 1s 942us/sample - loss: 4.1251 - val_loss: 4.7096\n",
            "Epoch 5/100\n",
            "1128/1128 [==============================] - 1s 949us/sample - loss: 3.7652 - val_loss: 4.4352\n",
            "Epoch 6/100\n",
            "1128/1128 [==============================] - 1s 950us/sample - loss: 3.3794 - val_loss: 4.2328\n",
            "Epoch 7/100\n",
            "1128/1128 [==============================] - 1s 950us/sample - loss: 2.9887 - val_loss: 4.0484\n",
            "Epoch 8/100\n",
            "1128/1128 [==============================] - 1s 937us/sample - loss: 2.6885 - val_loss: 3.7579\n",
            "Epoch 9/100\n",
            "1128/1128 [==============================] - 1s 941us/sample - loss: 2.4129 - val_loss: 3.5932\n",
            "Epoch 10/100\n",
            "1128/1128 [==============================] - 1s 946us/sample - loss: 2.2088 - val_loss: 3.6686\n",
            "Epoch 11/100\n",
            "1128/1128 [==============================] - 1s 938us/sample - loss: 2.0652 - val_loss: 3.4414\n",
            "Epoch 12/100\n",
            "1128/1128 [==============================] - 1s 938us/sample - loss: 1.8677 - val_loss: 3.3474\n",
            "Epoch 13/100\n",
            "1128/1128 [==============================] - 1s 938us/sample - loss: 1.7432 - val_loss: 3.3032\n",
            "Epoch 14/100\n",
            "1128/1128 [==============================] - 1s 939us/sample - loss: 1.6391 - val_loss: 3.2645\n",
            "Epoch 15/100\n",
            "1128/1128 [==============================] - 1s 955us/sample - loss: 1.5353 - val_loss: 3.1653\n",
            "Epoch 16/100\n",
            "1128/1128 [==============================] - 1s 958us/sample - loss: 1.4525 - val_loss: 3.0491\n",
            "Epoch 17/100\n",
            "1128/1128 [==============================] - 1s 941us/sample - loss: 1.3877 - val_loss: 3.0204\n",
            "Epoch 18/100\n",
            "1128/1128 [==============================] - 1s 958us/sample - loss: 1.3161 - val_loss: 3.1352\n",
            "Epoch 19/100\n",
            "1128/1128 [==============================] - 1s 949us/sample - loss: 1.2742 - val_loss: 2.9798\n",
            "Epoch 20/100\n",
            "1128/1128 [==============================] - 1s 944us/sample - loss: 1.2274 - val_loss: 3.0150\n",
            "Epoch 21/100\n",
            "1128/1128 [==============================] - 1s 944us/sample - loss: 1.1795 - val_loss: 2.9254\n",
            "Epoch 22/100\n",
            "1128/1128 [==============================] - 1s 953us/sample - loss: 1.1551 - val_loss: 2.8664\n",
            "Epoch 23/100\n",
            "1128/1128 [==============================] - 1s 960us/sample - loss: 1.1114 - val_loss: 2.9111\n",
            "Epoch 24/100\n",
            "1128/1128 [==============================] - 1s 966us/sample - loss: 1.0752 - val_loss: 2.8021\n",
            "Epoch 25/100\n",
            "1128/1128 [==============================] - 1s 954us/sample - loss: 1.0366 - val_loss: 2.8174\n",
            "Epoch 26/100\n",
            "1128/1128 [==============================] - 1s 960us/sample - loss: 1.0085 - val_loss: 2.8102\n",
            "Epoch 27/100\n",
            "1128/1128 [==============================] - 1s 960us/sample - loss: 0.9805 - val_loss: 2.8407\n",
            "Epoch 28/100\n",
            "1128/1128 [==============================] - 1s 964us/sample - loss: 0.9557 - val_loss: 2.7759\n",
            "Epoch 29/100\n",
            "1128/1128 [==============================] - 1s 964us/sample - loss: 0.9286 - val_loss: 2.7957\n",
            "Epoch 30/100\n",
            "1128/1128 [==============================] - 1s 972us/sample - loss: 0.9161 - val_loss: 2.8178\n",
            "Epoch 31/100\n",
            "1128/1128 [==============================] - 1s 975us/sample - loss: 0.8988 - val_loss: 2.7675\n",
            "Epoch 32/100\n",
            "1128/1128 [==============================] - 1s 964us/sample - loss: 0.8838 - val_loss: 2.7347\n",
            "Epoch 33/100\n",
            "1128/1128 [==============================] - 1s 970us/sample - loss: 0.8650 - val_loss: 2.7213\n",
            "Epoch 34/100\n",
            "1128/1128 [==============================] - 1s 978us/sample - loss: 0.8420 - val_loss: 2.6868\n",
            "Epoch 35/100\n",
            "1128/1128 [==============================] - 1s 983us/sample - loss: 0.8408 - val_loss: 2.6198\n",
            "Epoch 36/100\n",
            "1128/1128 [==============================] - 1s 979us/sample - loss: 0.8187 - val_loss: 2.7537\n",
            "Epoch 37/100\n",
            "1128/1128 [==============================] - 1s 975us/sample - loss: 0.8068 - val_loss: 2.7059\n",
            "Epoch 38/100\n",
            "1128/1128 [==============================] - 1s 959us/sample - loss: 0.7852 - val_loss: 2.7131\n",
            "Epoch 39/100\n",
            "1128/1128 [==============================] - 1s 982us/sample - loss: 0.7907 - val_loss: 2.6186\n",
            "Epoch 40/100\n",
            "1128/1128 [==============================] - 1s 982us/sample - loss: 0.7628 - val_loss: 2.6071\n",
            "Epoch 41/100\n",
            "1128/1128 [==============================] - 1s 970us/sample - loss: 0.7554 - val_loss: 2.6684\n",
            "Epoch 42/100\n",
            "1128/1128 [==============================] - 1s 974us/sample - loss: 0.7301 - val_loss: 2.6614\n",
            "Epoch 43/100\n",
            "1128/1128 [==============================] - 1s 965us/sample - loss: 0.7320 - val_loss: 2.6255\n",
            "Epoch 44/100\n",
            "1128/1128 [==============================] - 1s 972us/sample - loss: 0.7106 - val_loss: 2.5404\n",
            "Epoch 45/100\n",
            "1128/1128 [==============================] - 1s 981us/sample - loss: 0.7010 - val_loss: 2.5675\n",
            "Epoch 46/100\n",
            "1128/1128 [==============================] - 1s 977us/sample - loss: 0.6899 - val_loss: 2.5489\n",
            "Epoch 47/100\n",
            "1128/1128 [==============================] - 1s 979us/sample - loss: 0.6845 - val_loss: 2.6261\n",
            "Epoch 48/100\n",
            "1128/1128 [==============================] - 1s 978us/sample - loss: 0.6677 - val_loss: 2.5361\n",
            "Epoch 49/100\n",
            "1128/1128 [==============================] - 1s 977us/sample - loss: 0.6552 - val_loss: 2.5269\n",
            "Epoch 50/100\n",
            "1128/1128 [==============================] - 1s 976us/sample - loss: 0.6487 - val_loss: 2.5513\n",
            "Epoch 51/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 0.6377 - val_loss: 2.5480\n",
            "Epoch 52/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.6262 - val_loss: 2.4891\n",
            "Epoch 53/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.6286 - val_loss: 2.4955\n",
            "Epoch 54/100\n",
            "1128/1128 [==============================] - 1s 979us/sample - loss: 0.6208 - val_loss: 2.5314\n",
            "Epoch 55/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 0.6125 - val_loss: 2.5764\n",
            "Epoch 56/100\n",
            "1128/1128 [==============================] - 1s 982us/sample - loss: 0.6011 - val_loss: 2.5547\n",
            "Epoch 57/100\n",
            "1128/1128 [==============================] - 1s 977us/sample - loss: 0.6043 - val_loss: 2.4611\n",
            "Epoch 58/100\n",
            "1128/1128 [==============================] - 1s 982us/sample - loss: 0.6043 - val_loss: 2.4867\n",
            "Epoch 59/100\n",
            "1128/1128 [==============================] - 1s 989us/sample - loss: 0.5874 - val_loss: 2.4385\n",
            "Epoch 60/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5840 - val_loss: 2.4953\n",
            "Epoch 61/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5816 - val_loss: 2.4733\n",
            "Epoch 62/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.5762 - val_loss: 2.4511\n",
            "Epoch 63/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5639 - val_loss: 2.4720\n",
            "Epoch 64/100\n",
            "1128/1128 [==============================] - 1s 983us/sample - loss: 0.5616 - val_loss: 2.5465\n",
            "Epoch 65/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.5594 - val_loss: 2.4454\n",
            "Epoch 66/100\n",
            "1128/1128 [==============================] - 1s 1000us/sample - loss: 0.5604 - val_loss: 2.4906\n",
            "Epoch 67/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5532 - val_loss: 2.4355\n",
            "Epoch 68/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5346 - val_loss: 2.4368\n",
            "Epoch 69/100\n",
            "1128/1128 [==============================] - 1s 1000us/sample - loss: 0.5331 - val_loss: 2.4358\n",
            "Epoch 70/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5340 - val_loss: 2.4188\n",
            "Epoch 71/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5267 - val_loss: 2.3831\n",
            "Epoch 72/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5250 - val_loss: 2.4541\n",
            "Epoch 73/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5092 - val_loss: 2.4664\n",
            "Epoch 74/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5157 - val_loss: 2.4414\n",
            "Epoch 75/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5086 - val_loss: 2.3792\n",
            "Epoch 76/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5110 - val_loss: 2.3697\n",
            "Epoch 77/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4933 - val_loss: 2.4048\n",
            "Epoch 78/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4985 - val_loss: 2.4023\n",
            "Epoch 79/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4905 - val_loss: 2.3697\n",
            "Epoch 80/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4855 - val_loss: 2.3587\n",
            "Epoch 81/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4922 - val_loss: 2.2843\n",
            "Epoch 82/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4826 - val_loss: 2.4053\n",
            "Epoch 83/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4711 - val_loss: 2.3768\n",
            "Epoch 84/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4696 - val_loss: 2.3677\n",
            "Epoch 85/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4602 - val_loss: 2.3477\n",
            "Epoch 86/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4600 - val_loss: 2.3052\n",
            "Epoch 87/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4611 - val_loss: 2.3640\n",
            "Epoch 88/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4596 - val_loss: 2.3502\n",
            "Epoch 89/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4542 - val_loss: 2.3362\n",
            "Epoch 90/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4488 - val_loss: 2.3053\n",
            "Epoch 91/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4523 - val_loss: 2.3094\n",
            "Epoch 92/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4426 - val_loss: 2.3593\n",
            "Epoch 93/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4407 - val_loss: 2.3676\n",
            "Epoch 94/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4450 - val_loss: 2.3690\n",
            "Epoch 95/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4379 - val_loss: 2.3513\n",
            "Epoch 96/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4369 - val_loss: 2.4144\n",
            "Epoch 97/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4438 - val_loss: 2.3461\n",
            "Epoch 98/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4362 - val_loss: 2.3616\n",
            "Epoch 99/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4320 - val_loss: 2.3040\n",
            "Epoch 100/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.4280 - val_loss: 2.2642\n",
            "Test Corr: 0.6189188711787932\n",
            "Test RMSE: 1.3822247415497408\n",
            "Train Corr: 0.6189188711787932\n",
            "Train RMSE: 1.3822247415497408\n",
            "fold: 1\n",
            "Train on 1128 samples, validate on 126 samples\n",
            "Epoch 1/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 18.6861 - val_loss: 6.5694\n",
            "Epoch 2/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 7.1862 - val_loss: 7.7827\n",
            "Epoch 3/100\n",
            "1128/1128 [==============================] - 1s 991us/sample - loss: 8.1458 - val_loss: 8.5360\n",
            "Epoch 4/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 8.1228 - val_loss: 8.1140\n",
            "Epoch 5/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 7.3931 - val_loss: 7.5740\n",
            "Epoch 6/100\n",
            "1128/1128 [==============================] - 1s 983us/sample - loss: 6.6176 - val_loss: 7.0764\n",
            "Epoch 7/100\n",
            "1128/1128 [==============================] - 1s 985us/sample - loss: 5.8690 - val_loss: 6.4153\n",
            "Epoch 8/100\n",
            "1128/1128 [==============================] - 1s 985us/sample - loss: 5.2377 - val_loss: 5.8051\n",
            "Epoch 9/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 4.6650 - val_loss: 5.5405\n",
            "Epoch 10/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 4.1822 - val_loss: 5.1177\n",
            "Epoch 11/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 3.8157 - val_loss: 4.7754\n",
            "Epoch 12/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 3.4995 - val_loss: 4.5629\n",
            "Epoch 13/100\n",
            "1128/1128 [==============================] - 1s 986us/sample - loss: 3.2532 - val_loss: 4.3464\n",
            "Epoch 14/100\n",
            "1128/1128 [==============================] - 1s 979us/sample - loss: 3.0228 - val_loss: 4.2524\n",
            "Epoch 15/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 2.8678 - val_loss: 4.0277\n",
            "Epoch 16/100\n",
            "1128/1128 [==============================] - 1s 986us/sample - loss: 2.7028 - val_loss: 3.9130\n",
            "Epoch 17/100\n",
            "1128/1128 [==============================] - 1s 985us/sample - loss: 2.5617 - val_loss: 3.7990\n",
            "Epoch 18/100\n",
            "1128/1128 [==============================] - 1s 976us/sample - loss: 2.4072 - val_loss: 3.7419\n",
            "Epoch 19/100\n",
            "1128/1128 [==============================] - 1s 977us/sample - loss: 2.3132 - val_loss: 3.6765\n",
            "Epoch 20/100\n",
            "1128/1128 [==============================] - 1s 977us/sample - loss: 2.2087 - val_loss: 3.5661\n",
            "Epoch 21/100\n",
            "1128/1128 [==============================] - 1s 984us/sample - loss: 2.1027 - val_loss: 3.4797\n",
            "Epoch 22/100\n",
            "1128/1128 [==============================] - 1s 987us/sample - loss: 2.0343 - val_loss: 3.3664\n",
            "Epoch 23/100\n",
            "1128/1128 [==============================] - 1s 981us/sample - loss: 1.9440 - val_loss: 3.3498\n",
            "Epoch 24/100\n",
            "1128/1128 [==============================] - 1s 977us/sample - loss: 1.8767 - val_loss: 3.2653\n",
            "Epoch 25/100\n",
            "1128/1128 [==============================] - 1s 987us/sample - loss: 1.8101 - val_loss: 3.1953\n",
            "Epoch 26/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 1.7505 - val_loss: 3.1443\n",
            "Epoch 27/100\n",
            "1128/1128 [==============================] - 1s 990us/sample - loss: 1.6891 - val_loss: 3.0767\n",
            "Epoch 28/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 1.6385 - val_loss: 3.0740\n",
            "Epoch 29/100\n",
            "1128/1128 [==============================] - 1s 981us/sample - loss: 1.5919 - val_loss: 3.0132\n",
            "Epoch 30/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 1.5305 - val_loss: 2.9509\n",
            "Epoch 31/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 1.4987 - val_loss: 2.8832\n",
            "Epoch 32/100\n",
            "1128/1128 [==============================] - 1s 989us/sample - loss: 1.4605 - val_loss: 2.8490\n",
            "Epoch 33/100\n",
            "1128/1128 [==============================] - 1s 987us/sample - loss: 1.4153 - val_loss: 2.8596\n",
            "Epoch 34/100\n",
            "1128/1128 [==============================] - 1s 986us/sample - loss: 1.3925 - val_loss: 2.8465\n",
            "Epoch 35/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.3496 - val_loss: 2.7732\n",
            "Epoch 36/100\n",
            "1128/1128 [==============================] - 1s 988us/sample - loss: 1.3213 - val_loss: 2.7420\n",
            "Epoch 37/100\n",
            "1128/1128 [==============================] - 1s 1000us/sample - loss: 1.2879 - val_loss: 2.7837\n",
            "Epoch 38/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.2677 - val_loss: 2.7835\n",
            "Epoch 39/100\n",
            "1128/1128 [==============================] - 1s 986us/sample - loss: 1.2308 - val_loss: 2.7390\n",
            "Epoch 40/100\n",
            "1128/1128 [==============================] - 1s 991us/sample - loss: 1.2184 - val_loss: 2.6303\n",
            "Epoch 41/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 1.1921 - val_loss: 2.6862\n",
            "Epoch 42/100\n",
            "1128/1128 [==============================] - 1s 986us/sample - loss: 1.1635 - val_loss: 2.6507\n",
            "Epoch 43/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 1.1348 - val_loss: 2.6538\n",
            "Epoch 44/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 1.1232 - val_loss: 2.6214\n",
            "Epoch 45/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 1.0918 - val_loss: 2.5639\n",
            "Epoch 46/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 1.0712 - val_loss: 2.5593\n",
            "Epoch 47/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.0604 - val_loss: 2.6210\n",
            "Epoch 48/100\n",
            "1128/1128 [==============================] - 1s 989us/sample - loss: 1.0339 - val_loss: 2.5352\n",
            "Epoch 49/100\n",
            "1128/1128 [==============================] - 1s 989us/sample - loss: 1.0204 - val_loss: 2.4741\n",
            "Epoch 50/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 0.9978 - val_loss: 2.4477\n",
            "Epoch 51/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 0.9789 - val_loss: 2.4911\n",
            "Epoch 52/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.9660 - val_loss: 2.4502\n",
            "Epoch 53/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.9520 - val_loss: 2.4144\n",
            "Epoch 54/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.9367 - val_loss: 2.4352\n",
            "Epoch 55/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 0.9144 - val_loss: 2.4565\n",
            "Epoch 56/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 0.9094 - val_loss: 2.4150\n",
            "Epoch 57/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.9078 - val_loss: 2.4184\n",
            "Epoch 58/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.8885 - val_loss: 2.4004\n",
            "Epoch 59/100\n",
            "1128/1128 [==============================] - 1s 987us/sample - loss: 0.8747 - val_loss: 2.4165\n",
            "Epoch 60/100\n",
            "1128/1128 [==============================] - 1s 991us/sample - loss: 0.8677 - val_loss: 2.3742\n",
            "Epoch 61/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.8540 - val_loss: 2.3402\n",
            "Epoch 62/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.8446 - val_loss: 2.3545\n",
            "Epoch 63/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.8323 - val_loss: 2.3730\n",
            "Epoch 64/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 0.8140 - val_loss: 2.2912\n",
            "Epoch 65/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 0.8043 - val_loss: 2.3537\n",
            "Epoch 66/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.8011 - val_loss: 2.3103\n",
            "Epoch 67/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 0.7970 - val_loss: 2.3354\n",
            "Epoch 68/100\n",
            "1128/1128 [==============================] - 1s 989us/sample - loss: 0.7849 - val_loss: 2.2256\n",
            "Epoch 69/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 0.7751 - val_loss: 2.2830\n",
            "Epoch 70/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 0.7744 - val_loss: 2.2533\n",
            "Epoch 71/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7635 - val_loss: 2.3116\n",
            "Epoch 72/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7659 - val_loss: 2.2053\n",
            "Epoch 73/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 0.7538 - val_loss: 2.2640\n",
            "Epoch 74/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 0.7338 - val_loss: 2.2686\n",
            "Epoch 75/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.7314 - val_loss: 2.2628\n",
            "Epoch 76/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7207 - val_loss: 2.2414\n",
            "Epoch 77/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 0.7263 - val_loss: 2.2920\n",
            "Epoch 78/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7143 - val_loss: 2.2388\n",
            "Epoch 79/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 0.7188 - val_loss: 2.1339\n",
            "Epoch 80/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7179 - val_loss: 2.1595\n",
            "Epoch 81/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.6992 - val_loss: 2.2052\n",
            "Epoch 82/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 0.6870 - val_loss: 2.1592\n",
            "Epoch 83/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.6921 - val_loss: 2.1809\n",
            "Epoch 84/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.6794 - val_loss: 2.1262\n",
            "Epoch 85/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.6717 - val_loss: 2.2046\n",
            "Epoch 86/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.6554 - val_loss: 2.1592\n",
            "Epoch 87/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 0.6605 - val_loss: 2.1811\n",
            "Epoch 88/100\n",
            "1128/1128 [==============================] - 1s 983us/sample - loss: 0.6511 - val_loss: 2.1102\n",
            "Epoch 89/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 0.6435 - val_loss: 2.1407\n",
            "Epoch 90/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.6354 - val_loss: 2.0996\n",
            "Epoch 91/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.6246 - val_loss: 2.1207\n",
            "Epoch 92/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.6176 - val_loss: 2.1136\n",
            "Epoch 93/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 0.6176 - val_loss: 2.0748\n",
            "Epoch 94/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.6047 - val_loss: 2.0490\n",
            "Epoch 95/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 0.6037 - val_loss: 2.0749\n",
            "Epoch 96/100\n",
            "1128/1128 [==============================] - 1s 982us/sample - loss: 0.5993 - val_loss: 2.0484\n",
            "Epoch 97/100\n",
            "1128/1128 [==============================] - 1s 981us/sample - loss: 0.5897 - val_loss: 2.0947\n",
            "Epoch 98/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 0.5888 - val_loss: 2.1092\n",
            "Epoch 99/100\n",
            "1128/1128 [==============================] - 1s 989us/sample - loss: 0.5817 - val_loss: 2.0808\n",
            "Epoch 100/100\n",
            "1128/1128 [==============================] - 1s 986us/sample - loss: 0.5732 - val_loss: 1.9965\n",
            "Test Corr: 0.6845463536980128\n",
            "Test RMSE: 1.2253376336666173\n",
            "Train Corr: 0.6845463536980128\n",
            "Train RMSE: 1.2253376336666173\n",
            "fold: 2\n",
            "Train on 1128 samples, validate on 126 samples\n",
            "Epoch 1/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 13.6969 - val_loss: 6.0197\n",
            "Epoch 2/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 6.8887 - val_loss: 7.3859\n",
            "Epoch 3/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 7.7456 - val_loss: 7.7547\n",
            "Epoch 4/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 7.6427 - val_loss: 7.5783\n",
            "Epoch 5/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 6.9448 - val_loss: 6.8088\n",
            "Epoch 6/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 6.1389 - val_loss: 6.2844\n",
            "Epoch 7/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 5.4341 - val_loss: 6.0181\n",
            "Epoch 8/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 4.8511 - val_loss: 5.1413\n",
            "Epoch 9/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 4.3078 - val_loss: 4.7127\n",
            "Epoch 10/100\n",
            "1128/1128 [==============================] - 1s 990us/sample - loss: 3.8592 - val_loss: 4.5492\n",
            "Epoch 11/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 3.4787 - val_loss: 4.1278\n",
            "Epoch 12/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 3.1900 - val_loss: 3.9599\n",
            "Epoch 13/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 2.9283 - val_loss: 3.7875\n",
            "Epoch 14/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 2.7227 - val_loss: 3.5766\n",
            "Epoch 15/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 2.5462 - val_loss: 3.3735\n",
            "Epoch 16/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 2.4027 - val_loss: 3.2956\n",
            "Epoch 17/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 2.2758 - val_loss: 3.1811\n",
            "Epoch 18/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 2.1636 - val_loss: 3.0873\n",
            "Epoch 19/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 2.0614 - val_loss: 3.0520\n",
            "Epoch 20/100\n",
            "1128/1128 [==============================] - 1s 991us/sample - loss: 1.9868 - val_loss: 2.9543\n",
            "Epoch 21/100\n",
            "1128/1128 [==============================] - 1s 984us/sample - loss: 1.8700 - val_loss: 2.8506\n",
            "Epoch 22/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.8123 - val_loss: 2.7230\n",
            "Epoch 23/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.7377 - val_loss: 2.7270\n",
            "Epoch 24/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.6689 - val_loss: 2.6395\n",
            "Epoch 25/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.5999 - val_loss: 2.5522\n",
            "Epoch 26/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 1.5548 - val_loss: 2.6192\n",
            "Epoch 27/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 1.5041 - val_loss: 2.4835\n",
            "Epoch 28/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.4631 - val_loss: 2.4354\n",
            "Epoch 29/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.4117 - val_loss: 2.4104\n",
            "Epoch 30/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.3727 - val_loss: 2.3838\n",
            "Epoch 31/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.3476 - val_loss: 2.3736\n",
            "Epoch 32/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 1.3154 - val_loss: 2.3203\n",
            "Epoch 33/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 1.2882 - val_loss: 2.3272\n",
            "Epoch 34/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 1.2574 - val_loss: 2.2217\n",
            "Epoch 35/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 1.2126 - val_loss: 2.2411\n",
            "Epoch 36/100\n",
            "1128/1128 [==============================] - 1s 988us/sample - loss: 1.1946 - val_loss: 2.2029\n",
            "Epoch 37/100\n",
            "1128/1128 [==============================] - 1s 988us/sample - loss: 1.1609 - val_loss: 2.1673\n",
            "Epoch 38/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 1.1444 - val_loss: 2.1760\n",
            "Epoch 39/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 1.1087 - val_loss: 2.1743\n",
            "Epoch 40/100\n",
            "1128/1128 [==============================] - 1s 988us/sample - loss: 1.0877 - val_loss: 2.1444\n",
            "Epoch 41/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.0724 - val_loss: 2.1316\n",
            "Epoch 42/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 1.0477 - val_loss: 2.0602\n",
            "Epoch 43/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.0337 - val_loss: 2.1012\n",
            "Epoch 44/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 1.0198 - val_loss: 2.1673\n",
            "Epoch 45/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 1.0060 - val_loss: 2.0256\n",
            "Epoch 46/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 0.9750 - val_loss: 1.9919\n",
            "Epoch 47/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 0.9683 - val_loss: 2.0083\n",
            "Epoch 48/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 0.9535 - val_loss: 2.0348\n",
            "Epoch 49/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.9430 - val_loss: 1.9830\n",
            "Epoch 50/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.9207 - val_loss: 2.0315\n",
            "Epoch 51/100\n",
            "1128/1128 [==============================] - 1s 990us/sample - loss: 0.9127 - val_loss: 1.9846\n",
            "Epoch 52/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 0.8881 - val_loss: 2.0102\n",
            "Epoch 53/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 0.8721 - val_loss: 1.9974\n",
            "Epoch 54/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 0.8724 - val_loss: 1.9111\n",
            "Epoch 55/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 0.8514 - val_loss: 1.9036\n",
            "Epoch 56/100\n",
            "1128/1128 [==============================] - 1s 988us/sample - loss: 0.8426 - val_loss: 1.9469\n",
            "Epoch 57/100\n",
            "1128/1128 [==============================] - 1s 988us/sample - loss: 0.8350 - val_loss: 1.8844\n",
            "Epoch 58/100\n",
            "1128/1128 [==============================] - 1s 990us/sample - loss: 0.8226 - val_loss: 1.8605\n",
            "Epoch 59/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.8109 - val_loss: 1.8528\n",
            "Epoch 60/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 0.7954 - val_loss: 1.8856\n",
            "Epoch 61/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7904 - val_loss: 1.8271\n",
            "Epoch 62/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7923 - val_loss: 1.8790\n",
            "Epoch 63/100\n",
            "1128/1128 [==============================] - 1s 1000us/sample - loss: 0.7848 - val_loss: 1.8231\n",
            "Epoch 64/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 0.7619 - val_loss: 1.8777\n",
            "Epoch 65/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7664 - val_loss: 1.8101\n",
            "Epoch 66/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7514 - val_loss: 1.8496\n",
            "Epoch 67/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7397 - val_loss: 1.8076\n",
            "Epoch 68/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7340 - val_loss: 1.8603\n",
            "Epoch 69/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.7264 - val_loss: 1.8397\n",
            "Epoch 70/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7161 - val_loss: 1.7879\n",
            "Epoch 71/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 0.7068 - val_loss: 1.7697\n",
            "Epoch 72/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.6987 - val_loss: 1.7960\n",
            "Epoch 73/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7035 - val_loss: 1.7617\n",
            "Epoch 74/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.6967 - val_loss: 1.7240\n",
            "Epoch 75/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.6816 - val_loss: 1.7479\n",
            "Epoch 76/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 0.6702 - val_loss: 1.7953\n",
            "Epoch 77/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 0.6727 - val_loss: 1.7451\n",
            "Epoch 78/100\n",
            "1128/1128 [==============================] - 1s 990us/sample - loss: 0.6727 - val_loss: 1.7188\n",
            "Epoch 79/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.6663 - val_loss: 1.7519\n",
            "Epoch 80/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.6531 - val_loss: 1.7050\n",
            "Epoch 81/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 0.6409 - val_loss: 1.7400\n",
            "Epoch 82/100\n",
            "1128/1128 [==============================] - 1s 990us/sample - loss: 0.6387 - val_loss: 1.7007\n",
            "Epoch 83/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.6320 - val_loss: 1.6981\n",
            "Epoch 84/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.6278 - val_loss: 1.7213\n",
            "Epoch 85/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 0.6299 - val_loss: 1.6679\n",
            "Epoch 86/100\n",
            "1128/1128 [==============================] - 1s 991us/sample - loss: 0.6179 - val_loss: 1.6982\n",
            "Epoch 87/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 0.6073 - val_loss: 1.7091\n",
            "Epoch 88/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 0.6100 - val_loss: 1.7077\n",
            "Epoch 89/100\n",
            "1128/1128 [==============================] - 1s 989us/sample - loss: 0.6149 - val_loss: 1.6705\n",
            "Epoch 90/100\n",
            "1128/1128 [==============================] - 1s 990us/sample - loss: 0.6051 - val_loss: 1.7391\n",
            "Epoch 91/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 0.5996 - val_loss: 1.6753\n",
            "Epoch 92/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.5880 - val_loss: 1.6471\n",
            "Epoch 93/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.5834 - val_loss: 1.7105\n",
            "Epoch 94/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5823 - val_loss: 1.6291\n",
            "Epoch 95/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.5753 - val_loss: 1.6353\n",
            "Epoch 96/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 0.5718 - val_loss: 1.6555\n",
            "Epoch 97/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.5702 - val_loss: 1.6481\n",
            "Epoch 98/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5714 - val_loss: 1.6414\n",
            "Epoch 99/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5632 - val_loss: 1.6365\n",
            "Epoch 100/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.5592 - val_loss: 1.6453\n",
            "Test Corr: 0.7485249016925672\n",
            "Test RMSE: 1.082221017329028\n",
            "Train Corr: 0.7485249016925672\n",
            "Train RMSE: 1.082221017329028\n",
            "fold: 3\n",
            "Train on 1128 samples, validate on 126 samples\n",
            "Epoch 1/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 11.4986 - val_loss: 6.0426\n",
            "Epoch 2/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 6.9983 - val_loss: 7.6454\n",
            "Epoch 3/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 7.7061 - val_loss: 7.7760\n",
            "Epoch 4/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 7.5375 - val_loss: 7.3598\n",
            "Epoch 5/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 6.7754 - val_loss: 6.5996\n",
            "Epoch 6/100\n",
            "1128/1128 [==============================] - 1s 1000us/sample - loss: 5.9616 - val_loss: 5.9823\n",
            "Epoch 7/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 5.1654 - val_loss: 5.3944\n",
            "Epoch 8/100\n",
            "1128/1128 [==============================] - 1s 1000us/sample - loss: 4.5240 - val_loss: 5.0134\n",
            "Epoch 9/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 4.0379 - val_loss: 4.5773\n",
            "Epoch 10/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 3.6025 - val_loss: 4.2865\n",
            "Epoch 11/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 3.2499 - val_loss: 4.0776\n",
            "Epoch 12/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 2.9894 - val_loss: 3.9092\n",
            "Epoch 13/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 2.7603 - val_loss: 3.7303\n",
            "Epoch 14/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 2.5575 - val_loss: 3.5822\n",
            "Epoch 15/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 2.3789 - val_loss: 3.4466\n",
            "Epoch 16/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 2.2575 - val_loss: 3.3080\n",
            "Epoch 17/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 2.1059 - val_loss: 3.2399\n",
            "Epoch 18/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 2.0054 - val_loss: 3.2144\n",
            "Epoch 19/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.9075 - val_loss: 3.1169\n",
            "Epoch 20/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 1.8257 - val_loss: 2.9677\n",
            "Epoch 21/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 1.7445 - val_loss: 2.9705\n",
            "Epoch 22/100\n",
            "1128/1128 [==============================] - 1s 987us/sample - loss: 1.6761 - val_loss: 2.9121\n",
            "Epoch 23/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.6121 - val_loss: 2.8727\n",
            "Epoch 24/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 1.5621 - val_loss: 2.7764\n",
            "Epoch 25/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 1.5113 - val_loss: 2.6816\n",
            "Epoch 26/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 1.4556 - val_loss: 2.6180\n",
            "Epoch 27/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 1.4187 - val_loss: 2.6398\n",
            "Epoch 28/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 1.3625 - val_loss: 2.6303\n",
            "Epoch 29/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.3256 - val_loss: 2.5995\n",
            "Epoch 30/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.2872 - val_loss: 2.5450\n",
            "Epoch 31/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.2532 - val_loss: 2.4918\n",
            "Epoch 32/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.2243 - val_loss: 2.4999\n",
            "Epoch 33/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.1970 - val_loss: 2.4701\n",
            "Epoch 34/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.1774 - val_loss: 2.4266\n",
            "Epoch 35/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.1436 - val_loss: 2.4163\n",
            "Epoch 36/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.1218 - val_loss: 2.3868\n",
            "Epoch 37/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 1.0902 - val_loss: 2.4032\n",
            "Epoch 38/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.0699 - val_loss: 2.3434\n",
            "Epoch 39/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 1.0525 - val_loss: 2.3098\n",
            "Epoch 40/100\n",
            "1128/1128 [==============================] - 1s 991us/sample - loss: 1.0342 - val_loss: 2.3010\n",
            "Epoch 41/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 1.0080 - val_loss: 2.2494\n",
            "Epoch 42/100\n",
            "1128/1128 [==============================] - 1s 995us/sample - loss: 0.9965 - val_loss: 2.2927\n",
            "Epoch 43/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 0.9589 - val_loss: 2.2753\n",
            "Epoch 44/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.9503 - val_loss: 2.2540\n",
            "Epoch 45/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.9307 - val_loss: 2.2263\n",
            "Epoch 46/100\n",
            "1128/1128 [==============================] - 1s 992us/sample - loss: 0.9193 - val_loss: 2.2167\n",
            "Epoch 47/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.9031 - val_loss: 2.2192\n",
            "Epoch 48/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.9004 - val_loss: 2.1974\n",
            "Epoch 49/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.8861 - val_loss: 2.1904\n",
            "Epoch 50/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 0.8592 - val_loss: 2.1524\n",
            "Epoch 51/100\n",
            "1128/1128 [==============================] - 1s 993us/sample - loss: 0.8491 - val_loss: 2.1887\n",
            "Epoch 52/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.8406 - val_loss: 2.1484\n",
            "Epoch 53/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.8323 - val_loss: 2.1116\n",
            "Epoch 54/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.8292 - val_loss: 2.0742\n",
            "Epoch 55/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.8172 - val_loss: 2.0846\n",
            "Epoch 56/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.8068 - val_loss: 2.0473\n",
            "Epoch 57/100\n",
            "1128/1128 [==============================] - 1s 996us/sample - loss: 0.7861 - val_loss: 2.1051\n",
            "Epoch 58/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 0.7825 - val_loss: 2.0722\n",
            "Epoch 59/100\n",
            "1128/1128 [==============================] - 1s 999us/sample - loss: 0.7844 - val_loss: 2.0716\n",
            "Epoch 60/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7671 - val_loss: 2.0259\n",
            "Epoch 61/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7550 - val_loss: 2.0863\n",
            "Epoch 62/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7512 - val_loss: 1.9876\n",
            "Epoch 63/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7365 - val_loss: 2.0579\n",
            "Epoch 64/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7469 - val_loss: 1.9754\n",
            "Epoch 65/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 0.7279 - val_loss: 1.9486\n",
            "Epoch 66/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.7210 - val_loss: 1.9848\n",
            "Epoch 67/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 0.6953 - val_loss: 1.9911\n",
            "Epoch 68/100\n",
            "1128/1128 [==============================] - 1s 994us/sample - loss: 0.6903 - val_loss: 1.9548\n",
            "Epoch 69/100\n",
            "1128/1128 [==============================] - 1s 997us/sample - loss: 0.6850 - val_loss: 1.9377\n",
            "Epoch 70/100\n",
            "1128/1128 [==============================] - 1s 998us/sample - loss: 0.6787 - val_loss: 1.9662\n",
            "Epoch 71/100\n",
            "1128/1128 [==============================] - 1s 1ms/sample - loss: 0.6772 - val_loss: 1.9103\n",
            "Epoch 72/100\n",
            "1024/1128 [==========================>...] - ETA: 0s - loss: 0.6724"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4am6w3ASmRgx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB_a0klBHEpb",
        "colab_type": "text"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDY10ZF5HC2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LSTM 1層\n",
        "RMSE: 1.2745475772993313\n",
        "Corr 0.6560327096632175"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMt17zq44i3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6eCujp7ZZNh",
        "colab_type": "code",
        "outputId": "6cba611e-6a62-44e6-d2a1-2e2a8fede78c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4267463403034855 nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrp8eA2GmqVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        for i,(train, test) in enumerate(cv.split(X2)):\n",
        "          if i == 6 or i==7 or i ==8:\n",
        "              print('fold:',i)\n",
        "              model = NN_model()\n",
        "              #model.fit(X2[train],y2[train])\n",
        "              #predict = model.predict(X2[test])\n",
        "\n",
        "              model.fit(np.expand_dims(X2[train], axis=2),y2[train])\n",
        "              predict = model.predict(np.expand_dims(X2[test],axis=2))\n",
        "              predict = predict.flatten()\n",
        "              rmse = calcRMSE(y2[test],predict)\n",
        "              corr = calcCorr(y2[test],predict)\n",
        "              print('Corr:',corr)\n",
        "              print('RMSE:',rmse)\n",
        "              #score = model.evaluate(X2[test], y2[test])\n",
        "              score = model.evaluate(np.expand_dims(X2[test],axis=2), y2[test])\n",
        "              print(\"%s: %.2f%%\" % (model.metrics_names[0], score))\n",
        "          else:\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnFBBHNmXKh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def calcACC(testmodel,X=X,name=None):\n",
        "        def calcRMSE(real, pred):\n",
        "            RMSE = (np.sum((pred - real.tolist()) ** 2) / len(pred)) ** (1 / 2)\n",
        "            return RMSE\n",
        "\n",
        "        def calcCorr(real, pred):\n",
        "            corr = np.corrcoef(real, pred.flatten())[0, 1]\n",
        "            return corr\n",
        "        from sklearn.metrics import make_scorer\n",
        "        myScoreFunc = {'RMSE': make_scorer(calcRMSE),\n",
        "                       'Correlation coefficient': make_scorer(calcCorr)}\n",
        "        cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "        Scores = cross_validate(testmodel, X, y, cv=cv, scoring=myScoreFunc,return_train_score=True)\n",
        "        RMSETmp = Scores['test_RMSE'].mean()\n",
        "        CORRTmP = Scores['test_Correlation coefficient'].mean()\n",
        "        trainRMSETmp = Scores['train_RMSE'].mean()\n",
        "        trainCORRTmP = Scores['train_Correlation coefficient'].mean()\n",
        "        print(name,'test', RMSETmp, CORRTmP)\n",
        "        print(name,'train',trainRMSETmp, trainCORRTmP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgr1T6YMDn-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb7r_ENjAkc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a= sp.sparse.csc_matrix(X2.values)\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "class sparseNorm(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        Y = normalize(sp.sparse.csc_matrix(X.values))\n",
        "        return Y\n",
        "fm = sgd.FMRegression(n_iter=1000, init_stdev=0.1, l2_reg_w=0,l2_reg_V=0, rank=2, step_size=0.1)\n",
        "fm = sgd.FMRegression(\n",
        "            n_iter=4743,\n",
        "            init_stdev=0.1,\n",
        "            rank=100,\n",
        "            l2_reg_w=0,\n",
        "            l2_reg_V=0,\n",
        "            step_size=0.1,\n",
        "        )\n",
        "pipe = make_pipeline(sparseNorm(),fm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpcL4sOnDpJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "a= sp.sparse.csc_matrix(X2.values)\n",
        "sparseX = preprocessing.normalize(a)\n",
        "print(sparseX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhpXyReeJxyW",
        "colab_type": "code",
        "outputId": "f5a0789e-ee43-4db6-d010-b551a5fb2f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "def optFMRegression(n_iter, init_stdev, rank, l2_reg_w, l2_reg_V,step_size):\n",
        "    cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "    score = cross_validate(\n",
        "            sgd.FMRegression(\n",
        "            n_iter=int(n_iter),\n",
        "            init_stdev=init_stdev,\n",
        "            rank=int(rank),\n",
        "            l2_reg_w=l2_reg_w,\n",
        "            l2_reg_V=l2_reg_V,\n",
        "            step_size=step_size,\n",
        "        ),\n",
        "        sparseX, y,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        cv=cv,n_jobs=-1)\n",
        "    val = score['test_score'].mean()\n",
        "    return val\n",
        "score = optFMRegression(500,0,1,5,5,1)\n",
        "score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-inf"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFGIBmC3Go0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optFMRegression(n_iter, init_stdev, rank, l2_reg_w, l2_reg_V,step_size):\n",
        "    cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "    score = cross_validate(\n",
        "            sgd.FMRegression(\n",
        "            n_iter=int(n_iter),\n",
        "            init_stdev=init_stdev,\n",
        "            rank=int(rank),\n",
        "            l2_reg_w=l2_reg_w,\n",
        "            l2_reg_V=l2_reg_V,\n",
        "            step_size=step_size,\n",
        "        ),\n",
        "        sparseX, y,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        cv=cv,n_jobs=-1)\n",
        "    val = score['test_score'].mean()\n",
        "    return val\n",
        "\n",
        "opt = BayesianOptimization(\n",
        "        optFMRegression,\n",
        "        {\n",
        "            'n_iter': (500,5000),\n",
        "            'init_stdev' : (0,20),\n",
        "            'rank' : (1,100),\n",
        "            'l2_reg_w' : (0.1,1),\n",
        "            'l2_reg_V' : (0,0.1),\n",
        "            'step_size' : (0.01,1)}\n",
        "    )\n",
        "opt.maximize(init_points=10,n_iter=100)\n",
        "opt.max"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq_P5Q9jblBp",
        "colab_type": "code",
        "outputId": "5f70b604-a3d0-4894-d25e-fba79916de1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "X2.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>20.3</th>\n",
              "      <th>21.3</th>\n",
              "      <th>22.3</th>\n",
              "      <th>23.3</th>\n",
              "      <th>24.3</th>\n",
              "      <th>25.3</th>\n",
              "      <th>26.3</th>\n",
              "      <th>27.3</th>\n",
              "      <th>28.3</th>\n",
              "      <th>29.3</th>\n",
              "      <th>30.3</th>\n",
              "      <th>31.3</th>\n",
              "      <th>32.3</th>\n",
              "      <th>33.3</th>\n",
              "      <th>34.3</th>\n",
              "      <th>35.3</th>\n",
              "      <th>36.3</th>\n",
              "      <th>37.3</th>\n",
              "      <th>38.3</th>\n",
              "      <th>39.3</th>\n",
              "      <th>40.3</th>\n",
              "      <th>41.3</th>\n",
              "      <th>42.3</th>\n",
              "      <th>43.3</th>\n",
              "      <th>44.3</th>\n",
              "      <th>45.3</th>\n",
              "      <th>46.3</th>\n",
              "      <th>47.3</th>\n",
              "      <th>48.3</th>\n",
              "      <th>49.3</th>\n",
              "      <th>50.3</th>\n",
              "      <th>51.3</th>\n",
              "      <th>52.3</th>\n",
              "      <th>53.3</th>\n",
              "      <th>54.3</th>\n",
              "      <th>55.3</th>\n",
              "      <th>56.3</th>\n",
              "      <th>57.3</th>\n",
              "      <th>58.3</th>\n",
              "      <th>59.3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5610 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2  3  4  5  6  7  ...  52.3  53.3  54.3  55.3  56.3  57.3  58.3  59.3\n",
              "0  0  0  0  0  0  0  0  0  ...     0     0     1     0     0     0     0     0\n",
              "1  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0     0     0\n",
              "2  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0     0     0\n",
              "3  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0     0     0\n",
              "4  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0     0     0\n",
              "\n",
              "[5 rows x 5610 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpEk4_iSjbjH",
        "colab_type": "code",
        "outputId": "b4bb5b61-3f12-472a-bfd6-02dc3cb5dcf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "class sparseNorm(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        from sklearn import preprocessing\n",
        "        Y = preprocessing.normalize(sp.sparse.csc_matrix(X.values))\n",
        "        return Y\n",
        "fm = sgd.FMRegression(\n",
        "            n_iter=9943,\n",
        "            init_stdev=0.1,\n",
        "            rank=219,\n",
        "            l2_reg_w=0,\n",
        "            l2_reg_V=0.06454,\n",
        "            step_size=0.1,\n",
        "        )\n",
        "# fm = sgd.FMRegression(\n",
        "#             n_iter=4743,\n",
        "#             init_stdev=0.1,\n",
        "#             rank=100,\n",
        "#             l2_reg_w=0,\n",
        "#             l2_reg_V=0,\n",
        "#             step_size=0.1,\n",
        "#         )\n",
        "pipe = make_pipeline(sparseNorm(), fm)\n",
        "calcACC(pipe, X=X2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None test 1.2541720510947238 0.6584044433349703\n",
            "None train 1.0667708507173823 0.7909100866387624\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwliXvKmQluE",
        "colab_type": "code",
        "outputId": "eb60066b-7917-443c-ee04-70124550b9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def optFMRegression(n_iter,rank, l2_reg_w, l2_reg_V):\n",
        "    cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "    score = cross_validate(\n",
        "            sgd.FMRegression(\n",
        "            n_iter=n_iter,\n",
        "            init_stdev=0.1,\n",
        "            rank=int(rank),\n",
        "            l2_reg_w=l2_reg_w,\n",
        "            l2_reg_V=l2_reg_V,\n",
        "            step_size=0.1,\n",
        "        ),\n",
        "        sparseX, y,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        cv=cv,n_jobs=-1)\n",
        "    val = score['test_score'].mean()\n",
        "    return val\n",
        "\n",
        "opt = BayesianOptimization(\n",
        "        optFMRegression,\n",
        "        {\n",
        "            'n_iter': (1000,10000),\n",
        "            'rank' : (1,1000),\n",
        "            'l2_reg_w':(0,10),\n",
        "            'l2_reg_V':(0,10),\n",
        "            }\n",
        "    )\n",
        "opt.maximize(init_points=10,n_iter=100)\n",
        "opt.max"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | l2_reg_V  | l2_reg_w  |  n_iter   |   rank    |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.645   \u001b[0m | \u001b[0m 5.552   \u001b[0m | \u001b[0m 8.434   \u001b[0m | \u001b[0m 3.456e+0\u001b[0m | \u001b[0m 51.34   \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m-2.728   \u001b[0m | \u001b[0m 7.015   \u001b[0m | \u001b[0m 8.225   \u001b[0m | \u001b[0m 9.659e+0\u001b[0m | \u001b[0m 611.9   \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m-2.664   \u001b[0m | \u001b[0m 7.405   \u001b[0m | \u001b[0m 9.113   \u001b[0m | \u001b[0m 4.448e+0\u001b[0m | \u001b[0m 299.7   \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-2.676   \u001b[0m | \u001b[0m 5.971   \u001b[0m | \u001b[0m 6.375   \u001b[0m | \u001b[0m 5.296e+0\u001b[0m | \u001b[0m 180.9   \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-2.885   \u001b[0m | \u001b[0m 6.361   \u001b[0m | \u001b[0m 8.665   \u001b[0m | \u001b[0m 5.821e+0\u001b[0m | \u001b[0m 905.3   \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-2.821   \u001b[0m | \u001b[0m 2.463   \u001b[0m | \u001b[0m 5.221   \u001b[0m | \u001b[0m 5.173e+0\u001b[0m | \u001b[0m 982.6   \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m-2.879   \u001b[0m | \u001b[0m 7.909   \u001b[0m | \u001b[0m 5.632   \u001b[0m | \u001b[0m 9.255e+0\u001b[0m | \u001b[0m 486.2   \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.703   \u001b[0m | \u001b[0m 1.415   \u001b[0m | \u001b[0m 6.475   \u001b[0m | \u001b[0m 2.269e+0\u001b[0m | \u001b[0m 18.97   \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-2.73    \u001b[0m | \u001b[0m 4.203   \u001b[0m | \u001b[0m 4.202   \u001b[0m | \u001b[0m 1.961e+0\u001b[0m | \u001b[0m 151.7   \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-2.748   \u001b[0m | \u001b[0m 6.82    \u001b[0m | \u001b[0m 3.247   \u001b[0m | \u001b[0m 6.016e+0\u001b[0m | \u001b[0m 73.06   \u001b[0m |\n",
            "| \u001b[95m 11      \u001b[0m | \u001b[95m-1.66    \u001b[0m | \u001b[95m 1.736e-0\u001b[0m | \u001b[95m 2.189e-0\u001b[0m | \u001b[95m 1e+04   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m-2.756   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 1e+04   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-2.766   \u001b[0m | \u001b[0m 1.098   \u001b[0m | \u001b[0m 3.967   \u001b[0m | \u001b[0m 5.826e+0\u001b[0m | \u001b[0m 937.9   \u001b[0m |\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m-2.755   \u001b[0m | \u001b[0m 2.843   \u001b[0m | \u001b[0m 3.75    \u001b[0m | \u001b[0m 3.72e+03\u001b[0m | \u001b[0m 691.5   \u001b[0m |\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m-2.691   \u001b[0m | \u001b[0m 5.167   \u001b[0m | \u001b[0m 6.808   \u001b[0m | \u001b[0m 3.445e+0\u001b[0m | \u001b[0m 844.5   \u001b[0m |\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m-2.548   \u001b[0m | \u001b[0m 4.179   \u001b[0m | \u001b[0m 2.088   \u001b[0m | \u001b[0m 4.147e+0\u001b[0m | \u001b[0m 719.1   \u001b[0m |\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-2.693   \u001b[0m | \u001b[0m 3.221   \u001b[0m | \u001b[0m 6.403   \u001b[0m | \u001b[0m 7.882e+0\u001b[0m | \u001b[0m 30.81   \u001b[0m |\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m-2.885   \u001b[0m | \u001b[0m 6.765   \u001b[0m | \u001b[0m 0.8571  \u001b[0m | \u001b[0m 1.351e+0\u001b[0m | \u001b[0m 768.1   \u001b[0m |\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m-2.645   \u001b[0m | \u001b[0m 4.61    \u001b[0m | \u001b[0m 2.981   \u001b[0m | \u001b[0m 4.143e+0\u001b[0m | \u001b[0m 717.8   \u001b[0m |\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m-2.492   \u001b[0m | \u001b[0m 2.107   \u001b[0m | \u001b[0m 1.054   \u001b[0m | \u001b[0m 9.986e+0\u001b[0m | \u001b[0m 39.56   \u001b[0m |\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m-2.67    \u001b[0m | \u001b[0m 0.2611  \u001b[0m | \u001b[0m 3.185   \u001b[0m | \u001b[0m 9.955e+0\u001b[0m | \u001b[0m 13.02   \u001b[0m |\n",
            "| \u001b[95m 22      \u001b[0m | \u001b[95m-1.655   \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 4.193e+0\u001b[0m | \u001b[95m 735.7   \u001b[0m |\n",
            "| \u001b[95m 23      \u001b[0m | \u001b[95m-1.626   \u001b[0m | \u001b[95m 4.205   \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 4.201e+0\u001b[0m | \u001b[95m 704.6   \u001b[0m |\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m-2.987   \u001b[0m | \u001b[0m 6.665   \u001b[0m | \u001b[0m 4.089   \u001b[0m | \u001b[0m 4.238e+0\u001b[0m | \u001b[0m 733.9   \u001b[0m |\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m-2.67    \u001b[0m | \u001b[0m 1.987   \u001b[0m | \u001b[0m 9.248   \u001b[0m | \u001b[0m 4.201e+0\u001b[0m | \u001b[0m 667.3   \u001b[0m |\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m-2.679   \u001b[0m | \u001b[0m 8.536   \u001b[0m | \u001b[0m 6.423   \u001b[0m | \u001b[0m 4.171e+0\u001b[0m | \u001b[0m 765.6   \u001b[0m |\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m-2.706   \u001b[0m | \u001b[0m 3.676   \u001b[0m | \u001b[0m 4.166   \u001b[0m | \u001b[0m 2.889e+0\u001b[0m | \u001b[0m 724.7   \u001b[0m |\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m-2.739   \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 7.339   \u001b[0m | \u001b[0m 6.975e+0\u001b[0m | \u001b[0m 973.2   \u001b[0m |\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m-2.755   \u001b[0m | \u001b[0m 8.961   \u001b[0m | \u001b[0m 6.158   \u001b[0m | \u001b[0m 2.385e+0\u001b[0m | \u001b[0m 509.6   \u001b[0m |\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m-2.703   \u001b[0m | \u001b[0m 6.929   \u001b[0m | \u001b[0m 7.971   \u001b[0m | \u001b[0m 1.142e+0\u001b[0m | \u001b[0m 115.4   \u001b[0m |\n",
            "| \u001b[0m 31      \u001b[0m | \u001b[0m-2.649   \u001b[0m | \u001b[0m 0.9459  \u001b[0m | \u001b[0m 7.008   \u001b[0m | \u001b[0m 8.704e+0\u001b[0m | \u001b[0m 833.3   \u001b[0m |\n",
            "| \u001b[0m 32      \u001b[0m | \u001b[0m-3.24    \u001b[0m | \u001b[0m 3.825   \u001b[0m | \u001b[0m 2.301   \u001b[0m | \u001b[0m 6.883e+0\u001b[0m | \u001b[0m 155.2   \u001b[0m |\n",
            "| \u001b[0m 33      \u001b[0m | \u001b[0m-2.151   \u001b[0m | \u001b[0m 2.756   \u001b[0m | \u001b[0m 0.2703  \u001b[0m | \u001b[0m 9.997e+0\u001b[0m | \u001b[0m 164.4   \u001b[0m |\n",
            "| \u001b[0m 34      \u001b[0m | \u001b[0m-2.292   \u001b[0m | \u001b[0m 3.666   \u001b[0m | \u001b[0m 0.4978  \u001b[0m | \u001b[0m 9.977e+0\u001b[0m | \u001b[0m 211.5   \u001b[0m |\n",
            "| \u001b[0m 35      \u001b[0m | \u001b[0m-2.699   \u001b[0m | \u001b[0m 3.277   \u001b[0m | \u001b[0m 8.414   \u001b[0m | \u001b[0m 9.94e+03\u001b[0m | \u001b[0m 159.3   \u001b[0m |\n",
            "| \u001b[0m 36      \u001b[0m | \u001b[0m-2.814   \u001b[0m | \u001b[0m 4.649   \u001b[0m | \u001b[0m 9.865   \u001b[0m | \u001b[0m 9.997e+0\u001b[0m | \u001b[0m 115.7   \u001b[0m |\n",
            "| \u001b[0m 37      \u001b[0m | \u001b[0m-1.843   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1e+04   \u001b[0m | \u001b[0m 271.7   \u001b[0m |\n",
            "| \u001b[0m 38      \u001b[0m | \u001b[0m-2.942   \u001b[0m | \u001b[0m 7.689   \u001b[0m | \u001b[0m 9.307   \u001b[0m | \u001b[0m 9.958e+0\u001b[0m | \u001b[0m 282.7   \u001b[0m |\n",
            "| \u001b[0m 39      \u001b[0m | \u001b[0m-2.726   \u001b[0m | \u001b[0m 2.885   \u001b[0m | \u001b[0m 3.007   \u001b[0m | \u001b[0m 9.995e+0\u001b[0m | \u001b[0m 328.9   \u001b[0m |\n",
            "| \u001b[0m 40      \u001b[0m | \u001b[0m-2.494   \u001b[0m | \u001b[0m 7.695   \u001b[0m | \u001b[0m 0.6015  \u001b[0m | \u001b[0m 7.393e+0\u001b[0m | \u001b[0m 615.5   \u001b[0m |\n",
            "| \u001b[0m 41      \u001b[0m | \u001b[0m-3.118   \u001b[0m | \u001b[0m 4.764   \u001b[0m | \u001b[0m 2.573   \u001b[0m | \u001b[0m 7.443e+0\u001b[0m | \u001b[0m 678.6   \u001b[0m |\n",
            "| \u001b[0m 42      \u001b[0m | \u001b[0m-2.544   \u001b[0m | \u001b[0m 0.1861  \u001b[0m | \u001b[0m 4.849   \u001b[0m | \u001b[0m 7.345e+0\u001b[0m | \u001b[0m 557.4   \u001b[0m |\n",
            "| \u001b[95m 43      \u001b[0m | \u001b[95m-1.604   \u001b[0m | \u001b[95m 10.0    \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 7.429e+0\u001b[0m | \u001b[95m 542.9   \u001b[0m |\n",
            "| \u001b[0m 44      \u001b[0m | \u001b[0m-2.665   \u001b[0m | \u001b[0m 9.436   \u001b[0m | \u001b[0m 2.917   \u001b[0m | \u001b[0m 7.421e+0\u001b[0m | \u001b[0m 504.3   \u001b[0m |\n",
            "| \u001b[0m 45      \u001b[0m | \u001b[0m-3.093   \u001b[0m | \u001b[0m 1.609   \u001b[0m | \u001b[0m 5.888   \u001b[0m | \u001b[0m 7.459e+0\u001b[0m | \u001b[0m 568.2   \u001b[0m |\n",
            "| \u001b[0m 46      \u001b[0m | \u001b[0m-2.688   \u001b[0m | \u001b[0m 9.13    \u001b[0m | \u001b[0m 9.919   \u001b[0m | \u001b[0m 7.404e+0\u001b[0m | \u001b[0m 556.5   \u001b[0m |\n",
            "| \u001b[0m 47      \u001b[0m | \u001b[0m-2.676   \u001b[0m | \u001b[0m 5.719   \u001b[0m | \u001b[0m 4.173   \u001b[0m | \u001b[0m 4.181e+0\u001b[0m | \u001b[0m 712.9   \u001b[0m |\n",
            "| \u001b[0m 48      \u001b[0m | \u001b[0m-2.671   \u001b[0m | \u001b[0m 8.352   \u001b[0m | \u001b[0m 7.202   \u001b[0m | \u001b[0m 4.208e+0\u001b[0m | \u001b[0m 726.6   \u001b[0m |\n",
            "| \u001b[0m 49      \u001b[0m | \u001b[0m-3.099   \u001b[0m | \u001b[0m 4.834   \u001b[0m | \u001b[0m 9.203   \u001b[0m | \u001b[0m 7.438e+0\u001b[0m | \u001b[0m 530.9   \u001b[0m |\n",
            "| \u001b[0m 50      \u001b[0m | \u001b[0m-2.365   \u001b[0m | \u001b[0m 7.935   \u001b[0m | \u001b[0m 0.3249  \u001b[0m | \u001b[0m 7.431e+0\u001b[0m | \u001b[0m 559.5   \u001b[0m |\n",
            "| \u001b[0m 51      \u001b[0m | \u001b[0m-2.291   \u001b[0m | \u001b[0m 0.4433  \u001b[0m | \u001b[0m 0.4889  \u001b[0m | \u001b[0m 4.202e+0\u001b[0m | \u001b[0m 755.2   \u001b[0m |\n",
            "| \u001b[0m 52      \u001b[0m | \u001b[0m-2.705   \u001b[0m | \u001b[0m 9.696   \u001b[0m | \u001b[0m 9.074   \u001b[0m | \u001b[0m 4.214e+0\u001b[0m | \u001b[0m 692.3   \u001b[0m |\n",
            "| \u001b[0m 53      \u001b[0m | \u001b[0m-2.799   \u001b[0m | \u001b[0m 1.779   \u001b[0m | \u001b[0m 6.086   \u001b[0m | \u001b[0m 9.995e+0\u001b[0m | \u001b[0m 250.9   \u001b[0m |\n",
            "| \u001b[0m 54      \u001b[0m | \u001b[0m-2.796   \u001b[0m | \u001b[0m 1.183   \u001b[0m | \u001b[0m 4.364   \u001b[0m | \u001b[0m 9.992e+0\u001b[0m | \u001b[0m 290.9   \u001b[0m |\n",
            "| \u001b[0m 55      \u001b[0m | \u001b[0m-2.678   \u001b[0m | \u001b[0m 3.677   \u001b[0m | \u001b[0m 7.863   \u001b[0m | \u001b[0m 9.978e+0\u001b[0m | \u001b[0m 184.3   \u001b[0m |\n",
            "| \u001b[0m 56      \u001b[0m | \u001b[0m-2.61    \u001b[0m | \u001b[0m 4.58    \u001b[0m | \u001b[0m 2.732   \u001b[0m | \u001b[0m 7.413e+0\u001b[0m | \u001b[0m 534.7   \u001b[0m |\n",
            "| \u001b[95m 57      \u001b[0m | \u001b[95m-1.583   \u001b[0m | \u001b[95m 0.06455 \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 9.943e+0\u001b[0m | \u001b[95m 219.4   \u001b[0m |\n",
            "| \u001b[0m 58      \u001b[0m | \u001b[0m-1.779   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 9.923e+0\u001b[0m | \u001b[0m 217.7   \u001b[0m |\n",
            "| \u001b[0m 59      \u001b[0m | \u001b[0m-2.679   \u001b[0m | \u001b[0m 5.746   \u001b[0m | \u001b[0m 4.351   \u001b[0m | \u001b[0m 9.933e+0\u001b[0m | \u001b[0m 240.7   \u001b[0m |\n",
            "| \u001b[0m 60      \u001b[0m | \u001b[0m-2.689   \u001b[0m | \u001b[0m 2.046   \u001b[0m | \u001b[0m 8.953   \u001b[0m | \u001b[0m 9.934e+0\u001b[0m | \u001b[0m 198.1   \u001b[0m |\n",
            "| \u001b[0m 61      \u001b[0m | \u001b[0m-2.856   \u001b[0m | \u001b[0m 6.223   \u001b[0m | \u001b[0m 3.353   \u001b[0m | \u001b[0m 9.894e+0\u001b[0m | \u001b[0m 215.8   \u001b[0m |\n",
            "| \u001b[0m 62      \u001b[0m | \u001b[0m-2.718   \u001b[0m | \u001b[0m 4.617   \u001b[0m | \u001b[0m 9.688   \u001b[0m | \u001b[0m 7.343e+0\u001b[0m | \u001b[0m 627.0   \u001b[0m |\n",
            "| \u001b[0m 63      \u001b[0m | \u001b[0m-2.688   \u001b[0m | \u001b[0m 2.116   \u001b[0m | \u001b[0m 5.604   \u001b[0m | \u001b[0m 7.3e+03 \u001b[0m | \u001b[0m 519.7   \u001b[0m |\n",
            "| \u001b[0m 64      \u001b[0m | \u001b[0m-2.739   \u001b[0m | \u001b[0m 9.865   \u001b[0m | \u001b[0m 2.991   \u001b[0m | \u001b[0m 7.397e+0\u001b[0m | \u001b[0m 444.2   \u001b[0m |\n",
            "| \u001b[0m 65      \u001b[0m | \u001b[0m-2.987   \u001b[0m | \u001b[0m 8.456   \u001b[0m | \u001b[0m 2.696   \u001b[0m | \u001b[0m 4.634e+0\u001b[0m | \u001b[0m 996.5   \u001b[0m |\n",
            "| \u001b[0m 66      \u001b[0m | \u001b[0m-2.807   \u001b[0m | \u001b[0m 0.4902  \u001b[0m | \u001b[0m 9.892   \u001b[0m | \u001b[0m 1.132e+0\u001b[0m | \u001b[0m 418.4   \u001b[0m |\n",
            "| \u001b[0m 67      \u001b[0m | \u001b[0m-2.66    \u001b[0m | \u001b[0m 2.73    \u001b[0m | \u001b[0m 3.862   \u001b[0m | \u001b[0m 6.29e+03\u001b[0m | \u001b[0m 547.6   \u001b[0m |\n",
            "| \u001b[0m 68      \u001b[0m | \u001b[0m-2.709   \u001b[0m | \u001b[0m 4.568   \u001b[0m | \u001b[0m 6.687   \u001b[0m | \u001b[0m 6.391e+0\u001b[0m | \u001b[0m 206.6   \u001b[0m |\n",
            "| \u001b[0m 69      \u001b[0m | \u001b[0m-2.608   \u001b[0m | \u001b[0m 7.511   \u001b[0m | \u001b[0m 1.968   \u001b[0m | \u001b[0m 2.239e+0\u001b[0m | \u001b[0m 997.0   \u001b[0m |\n",
            "| \u001b[0m 70      \u001b[0m | \u001b[0m-1.709   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 9.961e+0\u001b[0m | \u001b[0m 231.9   \u001b[0m |\n",
            "| \u001b[0m 71      \u001b[0m | \u001b[0m-2.686   \u001b[0m | \u001b[0m 2.305   \u001b[0m | \u001b[0m 5.001   \u001b[0m | \u001b[0m 1.537e+0\u001b[0m | \u001b[0m 229.8   \u001b[0m |\n",
            "| \u001b[0m 72      \u001b[0m | \u001b[0m-2.729   \u001b[0m | \u001b[0m 8.739   \u001b[0m | \u001b[0m 8.876   \u001b[0m | \u001b[0m 2.752e+0\u001b[0m | \u001b[0m 181.1   \u001b[0m |\n",
            "| \u001b[0m 73      \u001b[0m | \u001b[0m-3.037   \u001b[0m | \u001b[0m 4.226   \u001b[0m | \u001b[0m 8.026   \u001b[0m | \u001b[0m 1.821e+0\u001b[0m | \u001b[0m 787.6   \u001b[0m |\n",
            "| \u001b[0m 74      \u001b[0m | \u001b[0m-2.686   \u001b[0m | \u001b[0m 4.016   \u001b[0m | \u001b[0m 3.772   \u001b[0m | \u001b[0m 2.182e+0\u001b[0m | \u001b[0m 904.9   \u001b[0m |\n",
            "| \u001b[0m 75      \u001b[0m | \u001b[0m-2.608   \u001b[0m | \u001b[0m 2.36    \u001b[0m | \u001b[0m 3.753   \u001b[0m | \u001b[0m 2.34e+03\u001b[0m | \u001b[0m 990.7   \u001b[0m |\n",
            "| \u001b[0m 76      \u001b[0m | \u001b[0m-2.471   \u001b[0m | \u001b[0m 8.594   \u001b[0m | \u001b[0m 0.664   \u001b[0m | \u001b[0m 2.41e+03\u001b[0m | \u001b[0m 924.7   \u001b[0m |\n",
            "| \u001b[0m 77      \u001b[0m | \u001b[0m-2.666   \u001b[0m | \u001b[0m 5.023   \u001b[0m | \u001b[0m 5.676   \u001b[0m | \u001b[0m 9.665e+0\u001b[0m | \u001b[0m 929.9   \u001b[0m |\n",
            "| \u001b[0m 78      \u001b[0m | \u001b[0m-2.678   \u001b[0m | \u001b[0m 8.887   \u001b[0m | \u001b[0m 4.765   \u001b[0m | \u001b[0m 2.46e+03\u001b[0m | \u001b[0m 915.8   \u001b[0m |\n",
            "| \u001b[0m 79      \u001b[0m | \u001b[0m-3.193   \u001b[0m | \u001b[0m 5.982   \u001b[0m | \u001b[0m 7.395   \u001b[0m | \u001b[0m 2.368e+0\u001b[0m | \u001b[0m 895.1   \u001b[0m |\n",
            "| \u001b[0m 80      \u001b[0m | \u001b[0m-2.672   \u001b[0m | \u001b[0m 9.235   \u001b[0m | \u001b[0m 6.619   \u001b[0m | \u001b[0m 2.401e+0\u001b[0m | \u001b[0m 975.0   \u001b[0m |\n",
            "| \u001b[0m 81      \u001b[0m | \u001b[0m-2.751   \u001b[0m | \u001b[0m 7.872   \u001b[0m | \u001b[0m 9.339   \u001b[0m | \u001b[0m 4.892e+0\u001b[0m | \u001b[0m 519.4   \u001b[0m |\n",
            "| \u001b[0m 82      \u001b[0m | \u001b[0m-2.697   \u001b[0m | \u001b[0m 5.572   \u001b[0m | \u001b[0m 4.169   \u001b[0m | \u001b[0m 9.086e+0\u001b[0m | \u001b[0m 827.7   \u001b[0m |\n",
            "| \u001b[0m 83      \u001b[0m | \u001b[0m-2.675   \u001b[0m | \u001b[0m 9.604   \u001b[0m | \u001b[0m 8.93    \u001b[0m | \u001b[0m 8.453e+0\u001b[0m | \u001b[0m 339.4   \u001b[0m |\n",
            "| \u001b[0m 84      \u001b[0m | \u001b[0m-2.705   \u001b[0m | \u001b[0m 3.506   \u001b[0m | \u001b[0m 7.557   \u001b[0m | \u001b[0m 4.115e+0\u001b[0m | \u001b[0m 145.1   \u001b[0m |\n",
            "| \u001b[0m 85      \u001b[0m | \u001b[0m-2.784   \u001b[0m | \u001b[0m 2.774   \u001b[0m | \u001b[0m 7.46    \u001b[0m | \u001b[0m 5.654e+0\u001b[0m | \u001b[0m 536.9   \u001b[0m |\n",
            "| \u001b[0m 86      \u001b[0m | \u001b[0m-2.724   \u001b[0m | \u001b[0m 7.228   \u001b[0m | \u001b[0m 6.247   \u001b[0m | \u001b[0m 7.958e+0\u001b[0m | \u001b[0m 854.3   \u001b[0m |\n",
            "| \u001b[0m 87      \u001b[0m | \u001b[0m-2.749   \u001b[0m | \u001b[0m 5.759   \u001b[0m | \u001b[0m 8.033   \u001b[0m | \u001b[0m 4.929e+0\u001b[0m | \u001b[0m 30.22   \u001b[0m |\n",
            "| \u001b[0m 88      \u001b[0m | \u001b[0m-2.683   \u001b[0m | \u001b[0m 0.3214  \u001b[0m | \u001b[0m 9.343   \u001b[0m | \u001b[0m 8.881e+0\u001b[0m | \u001b[0m 294.2   \u001b[0m |\n",
            "| \u001b[0m 89      \u001b[0m | \u001b[0m-2.865   \u001b[0m | \u001b[0m 8.659   \u001b[0m | \u001b[0m 1.609   \u001b[0m | \u001b[0m 6.868e+0\u001b[0m | \u001b[0m 541.9   \u001b[0m |\n",
            "| \u001b[0m 90      \u001b[0m | \u001b[0m-2.663   \u001b[0m | \u001b[0m 3.313   \u001b[0m | \u001b[0m 8.242   \u001b[0m | \u001b[0m 2.169e+0\u001b[0m | \u001b[0m 997.5   \u001b[0m |\n",
            "| \u001b[0m 91      \u001b[0m | \u001b[0m-2.604   \u001b[0m | \u001b[0m 7.782   \u001b[0m | \u001b[0m 2.107   \u001b[0m | \u001b[0m 6.551e+0\u001b[0m | \u001b[0m 841.0   \u001b[0m |\n",
            "| \u001b[0m 92      \u001b[0m | \u001b[0m-2.713   \u001b[0m | \u001b[0m 3.807   \u001b[0m | \u001b[0m 9.024   \u001b[0m | \u001b[0m 6.546e+0\u001b[0m | \u001b[0m 911.3   \u001b[0m |\n",
            "| \u001b[0m 93      \u001b[0m | \u001b[0m-3.095   \u001b[0m | \u001b[0m 6.845   \u001b[0m | \u001b[0m 3.216   \u001b[0m | \u001b[0m 6.522e+0\u001b[0m | \u001b[0m 777.4   \u001b[0m |\n",
            "| \u001b[0m 94      \u001b[0m | \u001b[0m-2.654   \u001b[0m | \u001b[0m 4.022   \u001b[0m | \u001b[0m 5.476   \u001b[0m | \u001b[0m 6.619e+0\u001b[0m | \u001b[0m 832.7   \u001b[0m |\n",
            "| \u001b[0m 95      \u001b[0m | \u001b[0m-2.649   \u001b[0m | \u001b[0m 7.096   \u001b[0m | \u001b[0m 5.121   \u001b[0m | \u001b[0m 7.841e+0\u001b[0m | \u001b[0m 458.7   \u001b[0m |\n",
            "| \u001b[0m 96      \u001b[0m | \u001b[0m-2.89    \u001b[0m | \u001b[0m 8.321   \u001b[0m | \u001b[0m 8.99    \u001b[0m | \u001b[0m 3.117e+0\u001b[0m | \u001b[0m 214.8   \u001b[0m |\n",
            "| \u001b[0m 97      \u001b[0m | \u001b[0m-2.824   \u001b[0m | \u001b[0m 1.576   \u001b[0m | \u001b[0m 7.748   \u001b[0m | \u001b[0m 9.319e+0\u001b[0m | \u001b[0m 110.4   \u001b[0m |\n",
            "| \u001b[0m 98      \u001b[0m | \u001b[0m-2.644   \u001b[0m | \u001b[0m 6.232   \u001b[0m | \u001b[0m 4.773   \u001b[0m | \u001b[0m 7.255e+0\u001b[0m | \u001b[0m 89.15   \u001b[0m |\n",
            "| \u001b[0m 99      \u001b[0m | \u001b[0m-2.788   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 2.268e+0\u001b[0m | \u001b[0m 936.3   \u001b[0m |\n",
            "| \u001b[0m 100     \u001b[0m | \u001b[0m-2.243   \u001b[0m | \u001b[0m 4.503   \u001b[0m | \u001b[0m 0.354   \u001b[0m | \u001b[0m 8.677e+0\u001b[0m | \u001b[0m 8.522   \u001b[0m |\n",
            "| \u001b[0m 101     \u001b[0m | \u001b[0m-2.66    \u001b[0m | \u001b[0m 8.569   \u001b[0m | \u001b[0m 3.415   \u001b[0m | \u001b[0m 8.641e+0\u001b[0m | \u001b[0m 18.69   \u001b[0m |\n",
            "| \u001b[0m 102     \u001b[0m | \u001b[0m-2.703   \u001b[0m | \u001b[0m 1.458   \u001b[0m | \u001b[0m 7.776   \u001b[0m | \u001b[0m 8.707e+0\u001b[0m | \u001b[0m 34.15   \u001b[0m |\n",
            "| \u001b[0m 103     \u001b[0m | \u001b[0m-2.687   \u001b[0m | \u001b[0m 3.06    \u001b[0m | \u001b[0m 5.631   \u001b[0m | \u001b[0m 5.265e+0\u001b[0m | \u001b[0m 607.7   \u001b[0m |\n",
            "| \u001b[0m 104     \u001b[0m | \u001b[0m-2.656   \u001b[0m | \u001b[0m 4.739   \u001b[0m | \u001b[0m 9.345   \u001b[0m | \u001b[0m 1.034e+0\u001b[0m | \u001b[0m 980.6   \u001b[0m |\n",
            "| \u001b[0m 105     \u001b[0m | \u001b[0m-2.576   \u001b[0m | \u001b[0m 1.429   \u001b[0m | \u001b[0m 1.34    \u001b[0m | \u001b[0m 3.444e+0\u001b[0m | \u001b[0m 450.4   \u001b[0m |\n",
            "| \u001b[0m 106     \u001b[0m | \u001b[0m-3.262   \u001b[0m | \u001b[0m 2.551   \u001b[0m | \u001b[0m 7.399   \u001b[0m | \u001b[0m 3.503e+0\u001b[0m | \u001b[0m 432.0   \u001b[0m |\n",
            "| \u001b[0m 107     \u001b[0m | \u001b[0m-2.899   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 9.955e+0\u001b[0m | \u001b[0m 219.7   \u001b[0m |\n",
            "| \u001b[0m 108     \u001b[0m | \u001b[0m-2.683   \u001b[0m | \u001b[0m 6.371   \u001b[0m | \u001b[0m 9.032   \u001b[0m | \u001b[0m 3.407e+0\u001b[0m | \u001b[0m 489.1   \u001b[0m |\n",
            "| \u001b[0m 109     \u001b[0m | \u001b[0m-2.483   \u001b[0m | \u001b[0m 2.218   \u001b[0m | \u001b[0m 0.742   \u001b[0m | \u001b[0m 3.399e+0\u001b[0m | \u001b[0m 416.8   \u001b[0m |\n",
            "| \u001b[0m 110     \u001b[0m | \u001b[0m-2.717   \u001b[0m | \u001b[0m 7.225   \u001b[0m | \u001b[0m 8.933   \u001b[0m | \u001b[0m 3.355e+0\u001b[0m | \u001b[0m 405.6   \u001b[0m |\n",
            "=========================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'params': {'l2_reg_V': 0.06454808266719353,\n",
              "  'l2_reg_w': 0.0,\n",
              "  'n_iter': 9943.385736317281,\n",
              "  'rank': 219.3830156897833},\n",
              " 'target': -1.5833112158687874}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkVXJozvIWZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = BayesianOptimization(\n",
        "        optFMRegression,\n",
        "        {\n",
        "            'n_iter':(500,5000),\n",
        "            'init_stdev':(0,20),\n",
        "            'rank':(1,100),\n",
        "            'l2_reg_w':(0,10),\n",
        "            'l2_reg_V':(0,10),\n",
        "            'step_size':(0.01,1)\n",
        "        }\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6gbpBvutd3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class addDim(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        Y = np.expand_dims(X, axis=2)\n",
        "        return Y\n",
        "def NN_model(data= X):\n",
        "  #data = np.expand_dims(data, axis=0)\n",
        "  #data = np.reshape(X_train.values, (-1, 13, 1))\n",
        "  inputs = Input(((data.shape[1],1)))\n",
        "#   print(inputs)\n",
        "  x = Conv1D(1,1,dilation_rate=2,padding=\"same\", activation=\"relu\")(inputs)\n",
        "  #x = BatchNormalization()(x)\n",
        "  #x = Conv1D(1,1,dilation_rate=2,padding=\"same\", activation=\"relu\")(x)\n",
        "\n",
        "  #x = MaxPooling1D(1, padding='same')(x)\n",
        "  #x = Conv1D(1,1,dilation_rate=2,padding=\"same\", activation=\"relu\")(x)\n",
        "  #x = MaxPooling1D(2, padding='same')(x)\n",
        "  #x = Conv1D(16,2,dilation_rate=2,padding=\"same\", activation=\"relu\")(x)\n",
        "  #x = MaxPooling1D(2, padding='same')(x)\n",
        "  #predictions = Conv1D(1,2,dilation_rate=2,padding=\"same\", activation=\"relu\")(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  #x = Dense(2048, activation=\"relu\")(x)\n",
        "#   x = Dropout(0.5)(x)\n",
        "#   x = Dense(2048, activation=\"relu\")(x)\n",
        "#   x = Dense(2048, activation=\"relu\")(x)\n",
        "#   x = Dense(512, activation=\"relu\")(x)\n",
        "#   x = Dense(256, activation=\"relu\")(x)\n",
        "#   x = Dense(64, activation=\"relu\")(x)\n",
        "#   x = Dense(16, activation=\"relu\")(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x  = Dense(2048, activation=\"relu\")(x)\n",
        "  predictions = Dense(1)(x)\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "  model.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
        "  #model.summary()\n",
        "  return model\n",
        "#nn_model = NN_model(X_train)\n",
        "nn = KerasRegressor(build_fn=NN_model, epochs=100, batch_size=100, verbose=0)\n",
        "nn = make_pipeline(addDim(),nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdsgzNVw0xHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class addDim(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        Y = np.expand_dims(X, axis=2)\n",
        "        return Y\n",
        "def LSTM_model(data= X):\n",
        "  #data = np.expand_dims(data, axis=0)\n",
        "  #data = np.reshape(X_train.values, (-1, 13, 1))\n",
        "  inputs = Input(((data.shape[1],1)))\n",
        "#   print(inputs)\n",
        "#   x = Conv1D(1,1,dilation_rate=2,padding=\"same\", activation=\"relu\")(inputs)\n",
        "#   x = BatchNormalization()(x)\n",
        "  #x = Conv1D(1,1,dilation_rate=2,padding=\"same\", activation=\"relu\")(x)\n",
        "  x = CuDNNLSTM(32,return_sequences=True)(inputs)\n",
        "  x = CuDNNLSTM(64,return_sequences=True)(x)\n",
        "  #x = MaxPooling1D(1, padding='same')(x)\n",
        "  #x = Conv1D(1,1,dilation_rate=2,padding=\"same\", activation=\"relu\")(x)\n",
        "  #x = MaxPooling1D(2, padding='same')(x)\n",
        "  #x = Conv1D(16,2,dilation_rate=2,padding=\"same\", activation=\"relu\")(x)\n",
        "  #x = MaxPooling1D(2, padding='same')(x)\n",
        "  #predictions = Conv1D(1,2,dilation_rate=2,padding=\"same\", activation=\"relu\")(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  #x = Dense(2048, activation=\"relu\")(x)\n",
        "#   x = Dropout(0.5)(x)\n",
        "#   x = Dense(2048, activation=\"relu\")(x)\n",
        "#   x = Dense(2048, activation=\"relu\")(x)\n",
        "#   x = Dense(512, activation=\"relu\")(x)\n",
        "#   x = Dense(256, activation=\"relu\")(x)\n",
        "#   x = Dense(64, activation=\"relu\")(x)\n",
        "#   x = Dense(16, activation=\"relu\")(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x  = Dense(128, activation=\"relu\")(x)\n",
        "  predictions = Dense(1)(x)\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "  model.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
        "  model.summary()\n",
        "  return model\n",
        "nn = KerasRegressor(build_fn=LSTM_model, epochs=100, batch_size=100, verbose=0)\n",
        "lstm = make_pipeline(addDim(),nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNPmnpM0XSOy",
        "colab_type": "code",
        "outputId": "76a4ec2d-1f92-400b-99da-b413e718a66a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "calcACC(lstm,X=X2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0626 15:45:38.144906 139993621567360 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 5610, 1)]         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm (CuDNNLSTM)       (None, 5610, 32)          4480      \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 5610, 64)          25088     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               45957248  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 45,986,945\n",
            "Trainable params: 45,986,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 5610, 1)]         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 5610, 32)          4480      \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_3 (CuDNNLSTM)     (None, 5610, 64)          25088     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               45957248  \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 45,986,945\n",
            "Trainable params: 45,986,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 5610, 1)]         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_4 (CuDNNLSTM)     (None, 5610, 32)          4480      \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_5 (CuDNNLSTM)     (None, 5610, 64)          25088     \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               45957248  \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 45,986,945\n",
            "Trainable params: 45,986,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 5610, 1)]         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_6 (CuDNNLSTM)     (None, 5610, 32)          4480      \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_7 (CuDNNLSTM)     (None, 5610, 64)          25088     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               45957248  \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 45,986,945\n",
            "Trainable params: 45,986,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 5610, 1)]         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_8 (CuDNNLSTM)     (None, 5610, 32)          4480      \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_9 (CuDNNLSTM)     (None, 5610, 64)          25088     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               45957248  \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 45,986,945\n",
            "Trainable params: 45,986,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 5610, 1)]         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_10 (CuDNNLSTM)    (None, 5610, 32)          4480      \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_11 (CuDNNLSTM)    (None, 5610, 64)          25088     \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               45957248  \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 45,986,945\n",
            "Trainable params: 45,986,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 5610, 1)]         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_12 (CuDNNLSTM)    (None, 5610, 32)          4480      \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_13 (CuDNNLSTM)    (None, 5610, 64)          25088     \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               45957248  \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 45,986,945\n",
            "Trainable params: 45,986,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 5610, 1)]         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_14 (CuDNNLSTM)    (None, 5610, 32)          4480      \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_15 (CuDNNLSTM)    (None, 5610, 64)          25088     \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 128)               45957248  \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 45,986,945\n",
            "Trainable params: 45,986,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 5610, 1)]         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_16 (CuDNNLSTM)    (None, 5610, 32)          4480      \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_17 (CuDNNLSTM)    (None, 5610, 64)          25088     \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 128)               45957248  \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 45,986,945\n",
            "Trainable params: 45,986,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 5610, 1)]         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_18 (CuDNNLSTM)    (None, 5610, 32)          4480      \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_19 (CuDNNLSTM)    (None, 5610, 64)          25088     \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 359040)            0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 128)               45957248  \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 45,986,945\n",
            "Trainable params: 45,986,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None test 1.441348223076561 0.5916109707677322\n",
            "None train 0.36219389600969476 0.9759812656746322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-gPS5Xx2MDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NN_model2(data= X2):\n",
        "  inputs = Input(shape=data.shape[1:])\n",
        "  #inputs = Input(shape=(7,))\n",
        "  x = Dense(2048, activation=\"relu\")(inputs)\n",
        "#   x = Dropout(0.5)(x)\n",
        "#   x = Dense(2048, activation=\"relu\")(x)\n",
        "#   x = Dense(2048, activation=\"relu\")(x)\n",
        "#   x = Dense(512, activation=\"relu\")(x)\n",
        "#   x = Dense(256, activation=\"relu\")(x)\n",
        "#   x = Dense(64, activation=\"relu\")(x)\n",
        "#   x = Dense(16, activation=\"relu\")(x)\n",
        "  #x = Dropout(0.3)(x)\n",
        "  x = Dense(2048, activation=\"relu\")(x)\n",
        "  predictions = Dense(1)(x)\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "  model.compile(optimizer=\"adam\",loss='mean_squared_error')\n",
        "  #model.summary()\n",
        "  return model\n",
        "#nn_model = NN_model2(X_train)\n",
        "#nn2 = KerasRegressor(build_fn=NN_model2, epochs=100, batch_size=10, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4jeYD_rp-8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import normalize \n",
        "\n",
        "class sparseNorm(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        Y = normalize(sp.sparse.csc_matrix(X.values))\n",
        "        return Y\n",
        "lgbm = LGBMRegressor(boosting_type='gbdt', num_leaves= 60,learning_rate=0.06)\n",
        "rgf = RGFRegressor(max_leaf=1000, algorithm=\"RGF\",test_interval=100, loss=\"LS\",verbose=False,l2=1.0)\n",
        "rgf1 = RGFRegressor(max_leaf=1000, algorithm=\"RGF\",test_interval=100, loss=\"LS\",verbose=False,l2=1.0)\n",
        "rgf2 = RGFRegressor(max_leaf=1000, algorithm=\"RGF\",test_interval=100, loss=\"LS\",verbose=False,l2=1.0)\n",
        "rgf3 = RGFRegressor(max_leaf=1000, algorithm=\"RGF\",test_interval=100, loss=\"LS\",verbose=False,l2=1.0)\n",
        "rgf4 = RGFRegressor(max_leaf=1000, algorithm=\"RGF\",test_interval=100, loss=\"LS\",verbose=False,l2=1.0)\n",
        "\n",
        "pipe1 = make_pipeline(extMACCS(), rgf)\n",
        "pipe2 = make_pipeline(extMorgan(), rgf1)\n",
        "pipe3 = make_pipeline(extDescriptor(), rgf2)\n",
        "pipe4 = make_pipeline(extPCA(), rgf3)\n",
        "pipe7 =make_pipeline(extDescriptor(), rgf4)\n",
        "pipe8 =make_pipeline(extDescriptor(), rgf4)\n",
        "from fastFM import sgd,als\n",
        "\n",
        "fm = sgd.FMRegression(n_iter=1000, init_stdev=0.1, l2_reg_w=0,l2_reg_V=0, rank=2, step_size=0.1)\n",
        "fmpipe = make_pipeline(sparseNorm(),fm)\n",
        "\n",
        "ave = extAverage()\n",
        "xgb = xgboost.XGBRegressor()\n",
        "nbrs = KNeighborsRegressor(2)\n",
        "svr = SVR(gamma='auto',kernel='linear')\n",
        "sgd = SGDRegressor(max_iter=1000)\n",
        "pls = PLSRegression(n_components=3)\n",
        "ext = ExtraTreesRegressor(n_estimators=30,max_features= 20,min_samples_split= 5,max_depth= 50, min_samples_leaf= 5)\n",
        "\n",
        "pipe5 = make_pipeline(extMorgan(), nbrs)\n",
        "pipe6 = make_pipeline(extMACCS(), rgf)\n",
        "alldata = make_pipeline(extAll())\n",
        "\n",
        "meta = RandomForestRegressor(max_depth=20, random_state=0, n_estimators=400)\n",
        "\n",
        "pipe1 = make_pipeline(extMACCS(), meta)\n",
        "pipe2 = make_pipeline(extMorgan(), meta)\n",
        "pipe3 = make_pipeline(extDescriptor(), meta)\n",
        "pipe4 = make_pipeline(extPCA(), rgf3)\n",
        "\n",
        "pipe7 =make_pipeline(extDescriptor(), rgf4)\n",
        "pipe8 =make_pipeline(extDescriptor(), rgf4)\n",
        "\n",
        "stack = StackingRegressor(regressors=[pipe1,pipe2,pipe3,rgf,xgb,lgbm,meta], meta_regressor=ave, verbose=1)\n",
        "\n",
        "stack1 = StackingRegressor(regressors=[pipe1, pipe2, pipe3,nn,fmpipe,nn2], meta_regressor=rgf, verbose=1)\n",
        "#stack2 = StackingRegressor(regressors=[stack1,nbrs, svr,pls,rgf], meta_regressor=lgbm, verbose=1)\n",
        "stack2 = StackingRegressor(regressors=[stack1,pipe5,pipe7,pipe1], meta_regressor=rgf,verbose=1)\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa_tnvihBu8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#0.70\n",
        "pipeRgf = make_pipeline(extMorgan(), rgf)\n",
        "pipeXgb = make_pipeline(extMorgan(), xgb)\n",
        "pipeLgbm = make_pipeline(extMorgan(), lgbm)\n",
        "pipeNbrs = make_pipeline(extMorgan(), nbrs)\n",
        "\n",
        "\n",
        "#stack = StackingRegressor(regressors=[pipe1,pipe2,pipe3,pipeRgf,pipeXgb,pipeLgbm], meta_regressor=ave, verbose=1)\n",
        "stack = StackingRegressor(regressors=[meta,rgf,xgb,lgbm], meta_regressor=ave, verbose=1)\n",
        "scores = cross_validate(stack, X, y, cv=cv)\n",
        "score = scores['test_score'].mean()**(1/2)\n",
        "score2 = scores['train_score'].mean()**(1/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tan0oj5tJB3F",
        "colab_type": "code",
        "outputId": "65b353de-6b87-48b6-da2e-769e3ee3e9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6992734576657249"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4muLHrjx2CjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#0.68\n",
        "pipePCA = make_pipeline(extPCA())\n",
        "\n",
        "stack1 = StackingRegressor(regressors=[pipe1, pipe2, pipe3], meta_regressor=meta, verbose=1)\n",
        "#0.68\n",
        "stack2 = StackingRegressor(regressors=[stack1,alldata,pipePCA,nbrs], meta_regressor=rgf,verbose=1)\n",
        "\n",
        "stack2 = StackingRegressor(regressors=[stack1,alldata,pipePCA,nbrs], meta_regressor=ave,verbose=1)\n",
        "scores2 = cross_validate(stack2, X, y, cv=cv)\n",
        "score3 = scores2['test_score'].mean()**(1/2)\n",
        "score4 = scores2['train_score'].mean()**(1/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLeRvW0u8_rG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stack1 = StackingRegressor(regressors=[pipe1, pipe2, pipe3], meta_regressor=meta, verbose=1)\n",
        "stack2 = StackingRegressor(regressors=[stack1,alldata], meta_regressor=meta,verbose=1)\n",
        "scores3 = cross_validate(stack2, X, y, cv=cv)\n",
        "score5 = scores3['test_score'].mean()**(1/2)\n",
        "score6 = scores3['train_score'].mean()**(1/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_peoRAEs8ugo",
        "colab_type": "code",
        "outputId": "cb227c32-e06a-43f8-82a6-66425a2e7e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXH1sJg9cNrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipePCA = make_pipeline(extPCA())\n",
        "\n",
        "stack3 = StackingRegressor(regressors=[pipe1, pipe2, pipe3], meta_regressor=meta, verbose=1)\n",
        "stack4 = StackingRegressor(regressors=[stack1,alldata,pipePCA], meta_regressor=rgf,verbose=1)\n",
        "scores10 = cross_validate(stack4, X, y, cv=cv)\n",
        "score11 = scores10['test_score'].mean()**(1/2)\n",
        "score12 = scores10['train_score'].mean()**(1/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd3aD3841Jie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ",rgf,xgb,lgbm,meta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7R8gPfIu9v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stack = StackingRegressor(regressors=[pipe1,pipe2,pipe3], meta_regressor=ave, verbose=1)\n",
        "scores = cross_validate(stack, X, y, cv=cv)\n",
        "score = scores['test_score'].mean()**(1/2)\n",
        "score2 = scores['train_score'].mean()**(1/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtPhiijEvcol",
        "colab_type": "code",
        "outputId": "5ac77dec-519c-4cc1-db61-99c673c6e547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6785203788102289"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SETgxXdtZjPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "singleResult = {}\n",
        "for name in [xgb,nbrs,svr,sgd,pls,ext,lgbm,rgf,meta,fmpipe]:\n",
        "  scores = cross_validate(name, X, y, cv=cv)\n",
        "  score = scores['test_score'].mean()**(1/2)\n",
        "  score2 = scores['train_score'].mean()**(1/2)\n",
        "  name2 = str(name)\n",
        "  singleResult[name] = (score,score2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwFuW-EAe8XW",
        "colab_type": "code",
        "outputId": "d2b364ef-7076-4876-8ab4-6ecc890e444f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
            "  warnings.warn(*warn_args, **warn_kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': array([0.02854085, 0.01813841, 0.01846361, 0.01905918, 0.01943231,\n",
              "        0.01915932, 0.01916122, 0.0187676 , 0.0185051 , 0.01845622]),\n",
              " 'score_time': array([0.00297284, 0.00268865, 0.00284624, 0.00341225, 0.00285077,\n",
              "        0.00278807, 0.00270033, 0.00278497, 0.00265265, 0.00293851]),\n",
              " 'test_score': array([0.09979622, 0.1815943 , 0.17128182, 0.18970238, 0.31300673,\n",
              "        0.25077943, 0.25094152, 0.29138371, 0.28893897, 0.06266754]),\n",
              " 'train_score': array([0.18613029, 0.12032096, 0.21351814, 0.22544877, 0.30175647,\n",
              "        0.24757158, 0.35747028, 0.36475411, 0.35926899, 0.23259555])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UHq2TlLdR_x",
        "colab_type": "code",
        "outputId": "eb74ef83-877f-402a-a121-2e5bcef7c085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        }
      },
      "source": [
        "singleResult"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=50,\n",
              "           max_features=20, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
              "           min_impurity_split=None, min_samples_leaf=5, min_samples_split=5,\n",
              "           min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
              "           oob_score=False, random_state=None, verbose=0, warm_start=False): (0.6231301383871978,\n",
              "  0.7680167562613793),\n",
              " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "           metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
              "           weights='uniform'): (0.5561273537929962, 0.8776402432176893),\n",
              " LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
              "        importance_type='split', learning_rate=0.06, max_depth=-1,\n",
              "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
              "        n_estimators=100, n_jobs=-1, num_leaves=60, objective=None,\n",
              "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
              "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0): (0.6740073058165706,\n",
              "  0.9385204557757585),\n",
              " PLSRegression(copy=True, max_iter=500, n_components=3, scale=True, tol=1e-06): (0.6087878772482173,\n",
              "  0.7387429922268596),\n",
              " Pipeline(memory=None,\n",
              "      steps=[('sparsenorm', sparseNorm()), ('fmregression', FMRegression(init_stdev=0.1, l2_reg=0, l2_reg_V=0, l2_reg_w=0, n_iter=1000,\n",
              "        random_state=123, rank=2, step_size=0.1))]): (0.45826767712319366,\n",
              "  0.5107675735399617),\n",
              " RGFRegressor(algorithm='RGF', init_model=None, l2=1.0, learning_rate=0.5,\n",
              "        loss='LS', max_leaf=1000, memory_policy='generous',\n",
              "        min_samples_leaf=10, n_iter=None, n_tree_search=1, normalize=True,\n",
              "        opt_interval=100, reg_depth=1.0, sl2=None, test_interval=100,\n",
              "        verbose=False): (0.6828725788699093, 0.849997190493384),\n",
              " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
              "            max_features='auto', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None,\n",
              "            oob_score=False, random_state=0, verbose=0, warm_start=False): (0.6938102422399695,\n",
              "  0.9585462892965368),\n",
              " SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
              "        eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
              "        learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
              "        n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,\n",
              "        random_state=None, shuffle=True, tol=None, validation_fraction=0.1,\n",
              "        verbose=0, warm_start=False): (0.3291565516025202, 0.8887763235337592),\n",
              " SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
              "   kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False): (nan,\n",
              "  0.860208752671646),\n",
              " XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "        colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
              "        max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
              "        n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
              "        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "        silent=True, subsample=1): (0.6795526862156849, 0.8340105881644)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNnYTUb9zDGz",
        "colab_type": "code",
        "outputId": "d36b13fe-5481-4e8a-83ea-5d3f816f9aee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "lstm.fit(X_train, y_train)\n",
        "y_pred = lstm.predict(X_train)\n",
        "y_val = lstm.predict(X_test)\n",
        "print(\"Root Mean Squared Error train: %.4f\" % calcRMSE(y_pred, y_train))\n",
        "print(\"Root Mean Squared Error test: %.4f\" % calcRMSE(y_val, y_test))\n",
        "print('Correlation Coefficient train: %.4f' % calcCorr(y_pred, y_train))\n",
        "print('Correlation Coefficient test: %.4f' % calcCorr(y_val, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_4 (CuDNNLSTM)     (None, 690, 224)          203392    \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_5 (CuDNNLSTM)     (None, 690, 384)          936960    \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 264960)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 264960)            0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               33915008  \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 35,055,489\n",
            "Trainable params: 35,055,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Root Mean Squared Error train: 0.2412\n",
            "Root Mean Squared Error test: 1.5216\n",
            "Correlation Coefficient train: 0.9898\n",
            "Correlation Coefficient test: 0.4670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSLJhyIfR-lj",
        "colab_type": "code",
        "outputId": "74139e80-3e76-4731-9a48-429d20a72deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "print(y_val,y_val.shape)\n",
        "y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125\n",
            " 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125 0.14148125] (126,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(126,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsT3T11nSKiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijUmrQYM1rAC",
        "colab_type": "code",
        "outputId": "f4c47a37-8b14-4f4a-f53c-b1be6099b8a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3378
        }
      },
      "source": [
        "scores = cross_validate(nn, X, y, cv=cv)\n",
        "scores['test_score'].mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_71 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_93 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_68 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_68 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_139 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_140 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_72 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_94 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_69 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_69 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_141 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_142 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_73 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_95 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_70 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_70 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_143 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_144 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_74 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_96 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_71 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_71 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_145 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_146 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_75 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_97 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_72 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_72 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_147 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_148 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_76 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_98 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_73 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_73 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_149 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_150 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_77 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_99 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_74 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_74 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_151 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_152 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_78 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_100 (Conv1D)          (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_75 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_75 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_153 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_154 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_79 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_101 (Conv1D)          (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_76 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_76 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_155 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_156 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_80 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_102 (Conv1D)          (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_77 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_77 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_157 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_158 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.55726209964071"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdI7qt1qyMEy",
        "colab_type": "code",
        "outputId": "bd6cf804-e962-4111-82d8-977a7373eff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
            "  warnings.warn(*warn_args, **warn_kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': array([17.53997922, 17.51591349, 17.62641382, 18.63325047, 18.35314131,\n",
              "        18.77135444, 19.03872037, 19.38783813, 20.17999578, 19.95681596]),\n",
              " 'score_time': array([2.63622236, 2.81975174, 2.88187885, 2.97793388, 3.18758726,\n",
              "        3.28418374, 3.39250493, 3.42442966, 3.53948808, 3.70629454]),\n",
              " 'test_score': array([-1.87397503, -1.48971084, -1.21239327, -1.56403474, -2.68844223,\n",
              "        -2.90050378, -2.04052038, -1.72630579, -1.75029235, -1.57858052]),\n",
              " 'train_score': array([-0.05194089, -1.44330622, -0.09889857, -1.31023485, -2.67425018,\n",
              "        -2.65073041, -1.82119017, -1.13014557, -0.08168194, -1.48566212])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3fKyQKbpbtA",
        "colab_type": "code",
        "outputId": "633345bf-7d8d-418a-c123-2a5615888625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8148
        }
      },
      "source": [
        "scores = cross_validate(stack2, X, y, cv=10)\n",
        "scores['test_score'].mean()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 regressors...\n",
            "Fitting regressor1: stackingregressor (1/4)\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_24 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_21 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_21 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_25 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor2: pipeline (2/4)\n",
            "Fitting regressor3: pipeline (3/4)\n",
            "Fitting regressor4: pipeline (4/4)\n",
            "Fitting 4 regressors...\n",
            "Fitting regressor1: stackingregressor (1/4)\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_26 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_22 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_22 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_27 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor2: pipeline (2/4)\n",
            "Fitting regressor3: pipeline (3/4)\n",
            "Fitting regressor4: pipeline (4/4)\n",
            "Fitting 4 regressors...\n",
            "Fitting regressor1: stackingregressor (1/4)\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_28 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_23 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_23 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_29 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor2: pipeline (2/4)\n",
            "Fitting regressor3: pipeline (3/4)\n",
            "Fitting regressor4: pipeline (4/4)\n",
            "Fitting 4 regressors...\n",
            "Fitting regressor1: stackingregressor (1/4)\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_30 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_24 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_24 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_31 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor2: pipeline (2/4)\n",
            "Fitting regressor3: pipeline (3/4)\n",
            "Fitting regressor4: pipeline (4/4)\n",
            "Fitting 4 regressors...\n",
            "Fitting regressor1: stackingregressor (1/4)\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_32 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_25 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_25 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_33 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor2: pipeline (2/4)\n",
            "Fitting regressor3: pipeline (3/4)\n",
            "Fitting regressor4: pipeline (4/4)\n",
            "Fitting 4 regressors...\n",
            "Fitting regressor1: stackingregressor (1/4)\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_34 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_26 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_26 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_35 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor2: pipeline (2/4)\n",
            "Fitting regressor3: pipeline (3/4)\n",
            "Fitting regressor4: pipeline (4/4)\n",
            "Fitting 4 regressors...\n",
            "Fitting regressor1: stackingregressor (1/4)\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_36 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_27 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_27 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_37 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor2: pipeline (2/4)\n",
            "Fitting regressor3: pipeline (3/4)\n",
            "Fitting regressor4: pipeline (4/4)\n",
            "Fitting 4 regressors...\n",
            "Fitting regressor1: stackingregressor (1/4)\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_38 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_28 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_28 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_84 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_39 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor2: pipeline (2/4)\n",
            "Fitting regressor3: pipeline (3/4)\n",
            "Fitting regressor4: pipeline (4/4)\n",
            "Fitting 4 regressors...\n",
            "Fitting regressor1: stackingregressor (1/4)\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_40 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_29 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_29 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_41 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_92 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor2: pipeline (2/4)\n",
            "Fitting regressor3: pipeline (3/4)\n",
            "Fitting regressor4: pipeline (4/4)\n",
            "Fitting 4 regressors...\n",
            "Fitting regressor1: stackingregressor (1/4)\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_42 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_30 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_30 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_43 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_97 (Dense)             (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor2: pipeline (2/4)\n",
            "Fitting regressor3: pipeline (3/4)\n",
            "Fitting regressor4: pipeline (4/4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.19763957, 0.28065453, 0.37411911, 0.39152718, 0.4430288 ,\n",
              "       0.48997388, 0.60091479, 0.48158644, 0.43197961, 0.22167769])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyQie9B4w8Ov",
        "colab_type": "code",
        "outputId": "f5c6e8f6-ec85-4ef2-c752-27e5a4b0edc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scores.mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.39131016006993274"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QvGXiwqu9ge",
        "colab_type": "code",
        "outputId": "712981c1-2542-4a4c-f933-7c4b15da16f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7446
        }
      },
      "source": [
        "stack_scores =cross_validate(stack1, X, y, cv=10)\n",
        "stack_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_50 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_34 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_34 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_51 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_116 (Dense)            (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_117 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_52 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_35 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_35 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_118 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_119 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_53 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_122 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_54 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_36 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_36 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_123 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_124 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_55 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_125 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_126 (Dense)            (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_127 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_56 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_37 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_37 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_128 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_129 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_57 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_130 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_131 (Dense)            (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_132 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_58 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_38 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_38 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_133 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_134 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_59 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_136 (Dense)            (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_137 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_60 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_39 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_39 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_138 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_139 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_61 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_140 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_141 (Dense)            (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_142 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_62 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_40 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_40 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_143 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_144 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_63 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_145 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_146 (Dense)            (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_147 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_64 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_41 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_41 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_148 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_149 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_65 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_150 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_151 (Dense)            (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_152 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_66 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_42 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_42 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_153 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_154 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_67 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_155 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_156 (Dense)            (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_157 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting 6 regressors...\n",
            "Fitting regressor1: pipeline (1/6)\n",
            "Fitting regressor2: pipeline (2/6)\n",
            "Fitting regressor3: pipeline (3/6)\n",
            "Fitting regressor4: pipeline (4/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_68 (InputLayer)        (None, 690, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_43 (Conv1D)           (None, 690, 1)            2         \n",
            "_________________________________________________________________\n",
            "flatten_43 (Flatten)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_158 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_159 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 1,417,219\n",
            "Trainable params: 1,417,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting regressor5: pipeline (5/6)\n",
            "Fitting regressor6: kerasregressor (6/6)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_69 (InputLayer)        (None, 690)               0         \n",
            "_________________________________________________________________\n",
            "dense_160 (Dense)            (None, 2048)              1415168   \n",
            "_________________________________________________________________\n",
            "dense_161 (Dense)            (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_162 (Dense)            (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 5,613,569\n",
            "Trainable params: 5,613,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
            "  warnings.warn(*warn_args, **warn_kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': array([133.56943321, 135.46863294, 135.77038169, 136.78971791,\n",
              "        138.16432285, 139.50198388, 140.6641326 , 141.19012785,\n",
              "        143.97067618, 144.0953393 ]),\n",
              " 'score_time': array([0.35180521, 0.35023689, 0.35720897, 0.36406612, 0.35870647,\n",
              "        0.37927723, 0.38316083, 0.36888576, 0.38707685, 0.36656427]),\n",
              " 'test_score': array([0.30323613, 0.26691208, 0.33824475, 0.4089086 , 0.38580084,\n",
              "        0.47605946, 0.60830864, 0.5006599 , 0.40077139, 0.16564585]),\n",
              " 'train_score': array([0.983355  , 0.99007855, 0.98353844, 0.9835329 , 0.9840576 ,\n",
              "        0.98359359, 0.9835808 , 0.9813044 , 0.98336132, 0.98668905])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIRmnW9WyEjE",
        "colab_type": "code",
        "outputId": "d0d38fcf-be46-4761-a207-2c755c59cd28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stack_scores['test_score'].mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3854547633234017"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB3J9bAH0Zm5",
        "colab_type": "code",
        "outputId": "f579f8bd-0480-4077-a5f5-eae175cc6a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "rgf_scores=cross_validate(rgf, X, y, cv=10)\n",
        "print(rgf_scores)\n",
        "rgf_scores.mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.34539658 0.37707443 0.40949323 0.40390886 0.42384531 0.45379597\n",
            " 0.6097257  0.42279969 0.46854228 0.49224834]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4406830390639035"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}